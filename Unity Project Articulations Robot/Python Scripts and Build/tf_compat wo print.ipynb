{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import time, random, os\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode_experience():\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, goal):\n",
    "        self.memory += [(state, action, reward, next_state, done, goal)]\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory = []\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, goal_size, action_low=-1, action_high=1, gamma=0.98,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, tau=1e-3):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.goal_size = goal_size\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = gamma   # discount rate\n",
    "        self.memory = []\n",
    "        self.buffer_size = int(5e4)\n",
    "        self.a_learning_rate = actor_learning_rate\n",
    "        self.c_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.tau = tau # soft update\n",
    "        self.batch_size = 32\n",
    "        self.gradient_norm_clip = None\n",
    "        self._construct_nets()\n",
    "\n",
    "    def _construct_nets(self):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "\n",
    "        self.S = tf.compat.v1.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.S_ = tf.compat.v1.placeholder(tf.float32, [None, self.state_size], 'next_state')\n",
    "        self.G = tf.compat.v1.placeholder(tf.float32, [None, self.goal_size], 'goal')\n",
    "        self.D = tf.compat.v1.placeholder(tf.float32, [None, 1], 'done')\n",
    "        self.R = tf.compat.v1.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.compat.v1.variable_scope('Actor'):\n",
    "            self.a = self._build_a(self.S, self.G, scope='eval')\n",
    "            self.a_ = self._build_a(self.S_, self.G, scope='target')\n",
    "        with tf.compat.v1.variable_scope('Critic'):\n",
    "            self.q = self._build_c(self.S, self.a, self.G, scope='eval')\n",
    "            self.q_ = self._build_c(self.S_, self.a_, self.G, scope='target')\n",
    "\n",
    "        self.ae_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        self.soft_update_op = [[tf.compat.v1.assign(ta, (1 - self.tau) * ta + self.tau * ea), \n",
    "                                tf.compat.v1.assign(tc, (1 - self.tau) * tc + self.tau * ec)]\n",
    "                    for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + self.gamma * (1-self.D) * self.q_\n",
    "#         q_target = tf.clip_by_value(q_target, -1/(1-self.gamma), 0)\n",
    "\n",
    "        self.c_loss = tf.compat.v1.losses.mean_squared_error(q_target, self.q)\n",
    "        self.a_loss = - tf.reduce_mean(input_tensor=self.q)    # maximize the q\n",
    "\n",
    "        if self.gradient_norm_clip is not None:\n",
    "            c_optimizer = tf.compat.v1.train.AdamOptimizer(self.c_learning_rate)\n",
    "            c_gradients = c_optimizer.compute_gradients(self.c_loss, var_list=self.ce_params)\n",
    "            for i, (grad, var) in enumerate(c_gradients):\n",
    "                if grad is not None:\n",
    "                    c_gradients[i] = (tf.clip_by_norm(grad, self.gradient_norm_clip), var)\n",
    "            self.c_train = c_optimizer.apply_gradients(c_gradients)\n",
    "            a_optimizer = tf.compat.v1.train.AdamOptimizer(self.a_learning_rate)\n",
    "            a_gradients = c_optimizer.compute_gradients(self.a_loss, var_list=self.ae_params)\n",
    "            for i, (grad, var) in enumerate(a_gradients):\n",
    "                if grad is not None:\n",
    "                    a_gradients[i] = (tf.clip_by_norm(grad, self.gradient_norm_clip), var)\n",
    "            self.a_train = a_optimizer.apply_gradients(a_gradients)\n",
    "        else:\n",
    "            self.c_train = tf.compat.v1.train.AdamOptimizer(self.c_learning_rate).minimize(self.c_loss, var_list=self.ce_params)\n",
    "            self.a_train = tf.compat.v1.train.AdamOptimizer(self.a_learning_rate).minimize(self.a_loss, var_list=self.ae_params)\n",
    "\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    def _build_a(self, s, g, scope): # policy\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            net = tf.concat([s, g], 1)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            a = tf.compat.v1.layers.dense(net, self.action_size, tf.nn.tanh)\n",
    "            result = a * (self.action_high-self.action_low)/2 + (self.action_high+self.action_low)/2\n",
    "            return result\n",
    "\n",
    "    def _build_c(self, s, a, g, scope): # Q value\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            net = tf.concat([s, a, g], 1)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
    "            return tf.compat.v1.layers.dense(net, 1)\n",
    "\n",
    "    def choose_action(self, state, goal, variance): # normal distribution\n",
    "        action = self.sess.run(self.a, {self.S: state, self.G: goal})[0]\n",
    "        return np.clip(np.random.normal(action, variance), self.action_low, self.action_high)\n",
    "\n",
    "    def remember(self, ep_experience):\n",
    "        self.memory += ep_experience.memory\n",
    "        if len(self.memory) > self.buffer_size:\n",
    "            self.memory = self.memory[-self.buffer_size:] # empty the first memories\n",
    "\n",
    "    def replay(self, optimization_steps=1):\n",
    "        if len(self.memory) < self.batch_size: # if there's no enough transitions, do nothing\n",
    "            return 0, 0\n",
    "\n",
    "        a_losses = 0\n",
    "        c_losses = 0\n",
    "        for _ in range(optimization_steps):\n",
    "            minibatch = np.vstack(random.sample(self.memory, self.batch_size))\n",
    "            ss = np.vstack(minibatch[:,0])\n",
    "            acs = np.vstack(minibatch[:,1])\n",
    "            rs = np.vstack(minibatch[:,2])\n",
    "            nss = np.vstack(minibatch[:,3])\n",
    "            ds = np.vstack(minibatch[:,4])\n",
    "            gs = np.vstack(minibatch[:,5])\n",
    "            a_loss, _ = self.sess.run([self.a_loss, self.a_train],\n",
    "                                      {self.S: ss, self.G: gs})\n",
    "            c_loss, _ = self.sess.run([self.c_loss, self.c_train],\n",
    "                                      {self.S: ss, self.a: acs, self.R: rs,\n",
    "                                       self.S_: nss, self.D: ds, self.G: gs})\n",
    "            a_losses += a_loss\n",
    "            c_losses += c_loss\n",
    "\n",
    "        return a_losses/optimization_steps, c_losses/optimization_steps\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.sess.run(self.soft_update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 1.2.0-preview and communication version 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected to Unity environment with package version 1.2.0-preview and communication version 1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected new brain: TouchCube?team=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs.environment:Connected new brain: TouchCube?team=0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"TouchCube\"\n",
    "    \n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num agents  1\n",
      "action size  3\n",
      "state size  15\n",
      "Agent state looks like: \n",
      "[[ 0.2430497   0.          0.37310985  0.2430497  -0.83987963  0.37310985\n",
      "   0.          0.83987963  0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "brain_name = list(env.behavior_specs)[0]#env.brain_names[0]\n",
    "brain = env.behavior_specs[brain_name]#env.brains[brain_name]\n",
    "decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "# Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "\n",
    "num_agents = len(decision_steps)\n",
    "print(\"num agents \", num_agents)\n",
    "\n",
    "if brain.action_spec.discrete_size > 0:\n",
    "    for action, branch_size in enumerate(brain.action_spec.discrete_branches):\n",
    "        print(f\"Action number {action} has {branch_size} different options\")\n",
    "        state_size = branch_size\n",
    "\n",
    "action_size = brain.action_spec.continuous_size\n",
    "print(\"action size \", action_size)\n",
    "\n",
    "decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "state_size = len(decision_steps.obs[0][0])\n",
    "print(\"state size \", state_size)\n",
    "\n",
    "print(\"Agent state looks like: \\n{}\".format(decision_steps.obs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(state, goal):\n",
    "    hand = state[3:6]\n",
    "    dist = np.linalg.norm(hand-goal)*5 # range=5\n",
    "    done = False\n",
    "    reward = -1\n",
    "    if dist<=1:\n",
    "        done = True\n",
    "        reward = 1\n",
    "    return done, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "c:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:85: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:86: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:87: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  a = tf.compat.v1.layers.dense(net, self.action_size, tf.nn.tanh)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:95: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:96: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_18060\\1521190820.py:97: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.compat.v1.layers.dense(net, 1)\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(state_size= state_size - 3, action_size=action_size, goal_size=3, action_high=1,\n",
    "                  action_low=-1, actor_learning_rate=1e-3, critic_learning_rate=1e-3,\n",
    "                  tau=0.1\n",
    "                 )\n",
    "variance=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\core\\shape_base.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 success rate 1.00 ep_mean_r -28.45 exploration 1.29          Training time : 748.17 s\n"
     ]
    }
   ],
   "source": [
    "#set_global_determinism(seed=SEED)\n",
    "#env.rng = np.random.default_rng(seed=SEED)\n",
    "use_her = True # use hindsight experience replay or not\n",
    "num_epochs = 100\n",
    "num_episodes = 20 # number of episodes over which success rate is computed\n",
    "episode_length = 500\n",
    "optimization_steps = 40\n",
    "K = 8 # number of random future states\n",
    "\n",
    "a_losses = []\n",
    "c_losses = []\n",
    "ep_mean_r = []\n",
    "success_rate = []\n",
    "\n",
    "ep_experience = Episode_experience()\n",
    "ep_experience_her = Episode_experience()\n",
    "\n",
    "start = time.process_time()\n",
    "total_step = 0\n",
    "for i in range(num_epochs):\n",
    "    successes = 0\n",
    "    ep_total_r = 0\n",
    "    for n in range(num_episodes):\n",
    "        \n",
    "        env.reset()\n",
    "        brain_name = list(env.behavior_specs)[0]\n",
    "        brain = env.behavior_specs[brain_name]\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        \n",
    "        state = decision_steps.obs[0][0][3:]\n",
    "        goal = decision_steps.obs[0][0][:3]\n",
    "        \n",
    "        for ep_step in range(episode_length):\n",
    "            total_step += 1\n",
    "            action = agent.choose_action([state], [goal], variance)\n",
    "            \n",
    "            #print(\"action \",action) # -- [ 1.         -0.81721041 -1.          1.        ]\n",
    "            #print(\"env_action.continuous \", env_action.continuous) # -- [[-0.739338 -0.739338 -0.739338 -0.739338]]\n",
    "            \n",
    "            env_action = brain.action_spec.random_action(len(decision_steps))\n",
    "            for j in range(len(action)):\n",
    "                env_action.continuous[0][j] = action[j]\n",
    "            \n",
    "            env.set_actions(brain_name, env_action)\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            \n",
    "            for agent_id_decisions in decision_steps:\n",
    "                next_state = decision_steps.obs[0][0][3:]\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                next_state = terminal_steps.obs[0][0][3:]\n",
    "            \n",
    "            for agent_id_decisions in decision_steps:\n",
    "                reward = decision_steps.reward[0]\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                reward = terminal_steps.reward[0]\n",
    "            \n",
    "            dones = terminal_steps.interrupted\n",
    "            done=False\n",
    "            if (len(dones) > 0):\n",
    "                done = True\n",
    "            else:\n",
    "                done=False         \n",
    "            \n",
    "            ep_total_r += reward\n",
    "            ep_experience.add(state, action, reward, next_state, done, goal)\n",
    "            state = next_state\n",
    "            if total_step % 200 == 0 or done:\n",
    "                if use_her: # The strategy can be changed here\n",
    "                    for t in range(len(ep_experience.memory)):\n",
    "                        for _ in range(K):\n",
    "                            future = np.random.randint(t, len(ep_experience.memory))\n",
    "                            goal_ = ep_experience.memory[future][3][3:6] # next_state of future\n",
    "                            state_ = ep_experience.memory[t][0]\n",
    "                            action_ = ep_experience.memory[t][1]\n",
    "                            next_state_ = ep_experience.memory[t][3]\n",
    "                            done_, reward_ = reward_func(next_state_, goal_)\n",
    "                            ep_experience_her.add(state_, action_, reward_, next_state_, done_, goal_)\n",
    "                    agent.remember(ep_experience_her)\n",
    "                    ep_experience_her.clear()\n",
    "                agent.remember(ep_experience)\n",
    "                ep_experience.clear()\n",
    "                variance *= 0.9995\n",
    "                a_loss, c_loss = agent.replay(optimization_steps)\n",
    "                a_losses += [a_loss]\n",
    "                c_losses += [c_loss]\n",
    "                agent.update_target_net()\n",
    "            if done:\n",
    "                break\n",
    "        successes += reward>=0 and done\n",
    "\n",
    "    success_rate.append(successes/num_episodes)\n",
    "    ep_mean_r.append(ep_total_r/num_episodes)\n",
    "    print(\"\\repoch\", i+1, \"success rate %.2f\"%success_rate[-1], \"ep_mean_r %.2f\"%ep_mean_r[-1], 'exploration %.2f'%variance, end=' '*10)\n",
    "\n",
    "print(\"Training time : %.2f\"%(time.process_time()-start), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.saver.save(agent.sess, \"model/old_hand_hands3_step3_her.ckpt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "c:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:85: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:86: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:87: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  a = tf.compat.v1.layers.dense(net, self.action_size, tf.nn.tanh)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:95: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:96: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  net = tf.compat.v1.layers.dense(net, 64, tf.nn.relu)\n",
      "C:\\Users\\vika9\\AppData\\Local\\Temp\\ipykernel_9192\\1521190820.py:97: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.compat.v1.layers.dense(net, 1)\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(state_size= state_size - 3, action_size=action_size, goal_size=3, action_high=1,\n",
    "                  action_low=-1, actor_learning_rate=1e-3, critic_learning_rate=1e-3,\n",
    "                  tau=0.1\n",
    "                 )\n",
    "variance=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\core\\shape_base.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 success rate 0.55 ep_mean_r -318.00 exploration 4.25          "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_episodes = 20 # number of episodes over which success rate is computed\n",
    "episode_length = 500\n",
    "optimization_steps = 40\n",
    "\n",
    "a_losses_without_her = []\n",
    "c_losses_without_her = []\n",
    "ep_mean_r_without_her = []\n",
    "success_rate_without_her = []\n",
    "\n",
    "ep_experience = Episode_experience()\n",
    "\n",
    "#start = time.clock()\n",
    "start = time.process_time()\n",
    "\n",
    "total_step = 0\n",
    "for i in range(num_epochs):\n",
    "    successes = 0\n",
    "    ep_total_r = 0\n",
    "    for n in range(num_episodes):\n",
    "        \n",
    "        #env_info = env.reset(train_mode=True)[default_brain]\n",
    "        #state = env_info.states[0][3:]\n",
    "        #goal = env_info.states[0][:3]\n",
    "        \n",
    "        env.reset()\n",
    "        brain_name = list(env.behavior_specs)[0]\n",
    "        brain = env.behavior_specs[brain_name]\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        state = decision_steps.obs[0][0][3:]\n",
    "        goal = decision_steps.obs[0][0][:3]\n",
    "        \n",
    "        for ep_step in range(episode_length):\n",
    "            total_step += 1\n",
    "            action = agent.choose_action([state], [goal], variance)\n",
    "            \n",
    "            #env_info = env.step(action)[default_brain]\n",
    "            \n",
    "            env_action = brain.action_spec.random_action(len(decision_steps))\n",
    "            for j in range(len(action)):\n",
    "                env_action.continuous[0][j] = action[j]\n",
    "            env.set_actions(brain_name, env_action)\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            \n",
    "            #next_state = env_info.states[0][3:]\n",
    "            #reward = env_info.rewards[0]\n",
    "            #done = env_info.local_done[0]\n",
    "            \n",
    "            for agent_id_decisions in decision_steps:\n",
    "                next_state = decision_steps.obs[0][0][3:]\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                next_state = terminal_steps.obs[0][0][3:]\n",
    "            for agent_id_decisions in decision_steps:\n",
    "                reward = decision_steps.reward[0]\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                reward = terminal_steps.reward[0]\n",
    "            dones = terminal_steps.interrupted\n",
    "            done=False\n",
    "            if (len(dones) > 0):\n",
    "                done = True\n",
    "            else:\n",
    "                done=False         \n",
    "            \n",
    "            ep_total_r += reward\n",
    "            ep_experience.add(state, action, reward, next_state, done, goal)\n",
    "            state = next_state\n",
    "            if total_step % 200 == 0 or done:\n",
    "                agent.remember(ep_experience)\n",
    "                ep_experience.clear()\n",
    "                variance *= 0.9995\n",
    "                a_loss, c_loss = agent.replay(optimization_steps)\n",
    "                a_losses_without_her += [a_loss]\n",
    "                c_losses_without_her += [c_loss]\n",
    "                agent.update_target_net()\n",
    "            if done:\n",
    "                break\n",
    "        successes += reward>=0 and done\n",
    "\n",
    "    success_rate_without_her.append(successes/num_episodes)\n",
    "    ep_mean_r_without_her.append(ep_total_r/num_episodes)\n",
    "    print(\"\\repoch\", i+1, \"success rate %.2f\"%success_rate_without_her[-1], \"ep_mean_r %.2f\"%ep_mean_r_without_her[-1], 'exploration %.2f'%variance, end=' '*10)\n",
    "\n",
    "print(\"Training time : %.2f\"%(time.process_time()-start), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.saver.save(agent.sess, \"model/old_hand_hands3_step3_withoun_her.ckpt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/old_hand_hands3_step3_her.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/old_hand_hands3_step3_her.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 60\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/old_hand_hands3_step3_her.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    env.reset()\n",
    "        \n",
    "    brain_name = list(env.behavior_specs)[0]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "    state = decision_steps.obs[0][0][3:]\n",
    "    goal = decision_steps.obs[0][0][:3]\n",
    "    \n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action([state], [goal], 0)\n",
    "        env_action = brain.action_spec.random_action(len(decision_steps))\n",
    "        for j in range(len(action)):\n",
    "            env_action.continuous[0][j] = action[j]\n",
    "        env.set_actions(brain_name, env_action)\n",
    "        env.step()\n",
    "                \n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        \n",
    "        for agent_id_decisions in decision_steps:\n",
    "            next_state = decision_steps.obs[0][0][3:]\n",
    "        for agent_id_terminated in terminal_steps:\n",
    "            next_state = terminal_steps.obs[0][0][3:]\n",
    "        for agent_id_decisions in decision_steps:\n",
    "            reward = decision_steps.reward[0]\n",
    "        for agent_id_terminated in terminal_steps:\n",
    "            reward = terminal_steps.reward[0]\n",
    "\n",
    "        dones = terminal_steps.interrupted\n",
    "        done=False\n",
    "        if (len(dones) > 0):\n",
    "            done = True      \n",
    "        \n",
    "        r += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_mean_r, label=\"ddpg+her\")\n",
    "plt.plot(ep_mean_r_without_her, label=\"ddpg\")\n",
    "plt.legend()\n",
    "plt.title(\"epoch mean reward (over 20 episodes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(success_rate, label=\"ddpg+her\")\n",
    "plt.plot(success_rate_without_her, label=\"ddpg\")\n",
    "plt.legend()\n",
    "plt.title(\"epoch success rate (over 20 episodes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,5))\n",
    "plt.suptitle('ddpg+her losses', size=20)\n",
    "plt.subplot(121)\n",
    "plt.title('c_loss')\n",
    "plt.plot(c_losses)\n",
    "plt.subplot(122)\n",
    "plt.title('a_loss')\n",
    "plt.plot(a_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,5))\n",
    "plt.suptitle('ddpg losses', size=20)\n",
    "plt.subplot(121)\n",
    "plt.title('c_loss')\n",
    "plt.plot(c_losses_without_her)\n",
    "plt.subplot(122)\n",
    "plt.title('a_loss')\n",
    "plt.plot(a_losses_without_her)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
