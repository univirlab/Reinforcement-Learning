{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a1970a-70fc-4997-934e-e02558918bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\vika9\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q pyserial\n",
    "from MathTools import get_rotate_offset_position\n",
    "from MathTools import RotateAxis\n",
    "from DDPGAgent import DDPGAgent\n",
    "import numpy as np\n",
    "import warnings\n",
    "import serial, struct, time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af473b24-c39a-4eda-b10d-bb332c140e85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7206709d-3323-4a62-a7c1-822e7fa4a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Servo_lx16a:\n",
    "    \n",
    "    def __init__(self, ID, Port, Interlock = False):\n",
    "        self.ID = ID # unique servo ID\n",
    "        self.Port = Port # COM port features for later use \n",
    "        self.Interlock = Interlock # While Interlock = True, Servo will not move\n",
    "        \n",
    "    @staticmethod   \n",
    "    def checksum(msg):\n",
    "        #Checksum=~(ID+Length+Cmd+Prm1+...+PrmN)\n",
    "        #If the numbers in the brackets exceeds 255,\n",
    "        #Take the lowest one byte, \"~\" means Negation.\n",
    "        s = sum(msg)\n",
    "        s = ~s&255\n",
    "        chksum = s.to_bytes(1, byteorder ='little')\n",
    "        return chksum\n",
    "    \n",
    "    @staticmethod  \n",
    "    def display_msg(msg):\n",
    "        #dispalys bytes in python console not as ASCII symbols but as bytes\n",
    "        print(''.join(r'\\x'+hex(letter)[2:] for letter in msg))\n",
    "      \n",
    "    @staticmethod     \n",
    "    def time(current_position, target_position):\n",
    "        #currently not used\n",
    "        #range of time for Servo movement is 0~30000ms\n",
    "        #3000ms for 1000 steps\n",
    "        #y ms for N steps\n",
    "        #y = N*3000/1000\n",
    "        return abs(target_position-current_position)*3000/1000\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_angle(position):\n",
    "        #returns Servo angle from Servo position\n",
    "        #Range of position is 0..1000 which is equal to 0..240 deg\n",
    "        #240 degree equals to 1000 \n",
    "        #x degree equals to N\n",
    "        #x= (N*240/1000)\n",
    "        return position*240/1000\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_position(angle):\n",
    "        #return position from angle\n",
    "        return int(angle*1000/240)\n",
    "    \n",
    "    Header = b'\\x55\\x55' #every message transfered and recieved starts with this Header\n",
    "    \n",
    "    def ReadPosition(self):\n",
    "        #Reads current position of Servo \n",
    "        temp = self.ID+b'\\x03'+b'\\x1c' \n",
    "        msg = self.Header + temp + self.checksum(temp) #message to read Servo position\n",
    "        print(f'Reading position of servo {int.from_bytes(self.ID, \"little\")}, sent:')\n",
    "        self.display_msg(msg)\n",
    "\n",
    "        wait = True\n",
    "        data_in = b'' # receive answer in data_in while wait True\n",
    "        ser.write(msg) #send message to COM\n",
    "        while wait:\n",
    "            while ser.in_waiting:\n",
    "                data_in += ser.readline()\n",
    "                print('Received: ')\n",
    "                self.display_msg(data_in)\n",
    "                if len(data_in)>=8: wait = False\n",
    "        received_chksum = self.checksum(data_in[-6:-1]) \n",
    "        if data_in.startswith(self.Header): #check that received data_in startswith Header and that checksum is correct\n",
    "            print('Message Header correct')\n",
    "            if  (received_chksum == data_in[-1].to_bytes(1, 'big')):\n",
    "                print('Checksum correct')\n",
    "                position = struct.unpack('<h', data_in[-3:-1])[0]\n",
    "                angle = self.to_angle(position)\n",
    "                print(f'Current position {position}, {angle}')\n",
    "                print('---------------------------------------')\n",
    "                return angle\n",
    "            else:\n",
    "                print('Bad checksum')\n",
    "        else:\n",
    "            print('Bad message header')\n",
    "\n",
    "\n",
    "\n",
    "    def MoveServo(self, target_angle):\n",
    "        #Moves servo to traget angle\n",
    "        if self.Interlock:\n",
    "            print('Servo is Interlocked') #if Servo is interlocked, it will not move\n",
    "        else:\n",
    "            target_position = self.to_position(target_angle) \n",
    "            temp = self.ID+b'\\x07'+b'\\x01'+struct.pack('<h', target_position)+struct.pack('<h', 1500) #set time temporary to 1500 ms for any movement\n",
    "            msg = self.Header+temp+self.checksum(temp) # message to move servo to desired position\n",
    "            print(f'Move servo {int.from_bytes(self.ID, \"little\")} to {target_position}, sent:')\n",
    "            self.display_msg(msg)\n",
    "            self.Interlock = True #Interlock servo\n",
    "            ser.write(msg) #sending message to COM\n",
    "            #This while cycle switches Interlock to False, when Servo reaches target position\n",
    "            while self.Interlock:\n",
    "                current_angle = self.ReadPosition()\n",
    "                angle_diff = abs(current_angle-target_angle)\n",
    "                print('Angle_difference:', angle_diff)\n",
    "                print('---------------------------------------')\n",
    "                if angle_diff<0.8:\n",
    "                    self.Interlock=False\n",
    "                    print('Reached target angle +- 1 deg')\n",
    "                    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd80fec-4571-47e2-b763-4928456b92ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0151087040722454, 0.8343491981978997, 0.07299609626426051]\n"
     ]
    }
   ],
   "source": [
    "test_result = get_rotate_offset_position(RotateAxis.OY, 0, RotateAxis.OX, 5, RotateAxis.OZ, 2,\n",
    "                                             RotateAxis.OY, 0.4048796, [0, 0.8378, 0])\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099530fa-5b1e-44ee-b1ba-36433f457787",
   "metadata": {},
   "source": [
    "## Servo settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db9c7207-fba6-4198-a4d9-339f04649186",
   "metadata": {},
   "outputs": [
    {
     "ename": "SerialException",
     "evalue": "could not open port 'COM3': OSError(22, 'Превышен таймаут семафора.', None, 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSerialException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m com \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOM3\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#COM port id\u001b[39;00m\n\u001b[0;32m      6\u001b[0m baudrate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m115200\u001b[39m \u001b[38;5;66;03m#COM speed\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m ser \u001b[38;5;241m=\u001b[39m \u001b[43mserial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaudrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m Servo1 \u001b[38;5;241m=\u001b[39m Servo_lx16a(Servo1_ID, ser)\n\u001b[0;32m     10\u001b[0m Servo2 \u001b[38;5;241m=\u001b[39m Servo_lx16a(Servo2_ID, ser)\n",
      "File \u001b[1;32mc:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\serial\\serialwin32.py:33\u001b[0m, in \u001b[0;36mSerial.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSerial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\serial\\serialutil.py:244\u001b[0m, in \u001b[0;36mSerialBase.__init__\u001b[1;34m(self, port, baudrate, bytesize, parity, stopbits, timeout, xonxoff, rtscts, write_timeout, dsrdtr, inter_byte_timeout, exclusive, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munexpected keyword arguments: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(kwargs))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m port \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\vika9\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\serial\\serialwin32.py:64\u001b[0m, in \u001b[0;36mSerial.open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port_handle \u001b[38;5;241m==\u001b[39m win32\u001b[38;5;241m.\u001b[39mINVALID_HANDLE_VALUE:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m# 'cause __del__ is called anyway\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SerialException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not open port \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mportstr, ctypes\u001b[38;5;241m.\u001b[39mWinError()))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_read \u001b[38;5;241m=\u001b[39m win32\u001b[38;5;241m.\u001b[39mOVERLAPPED()\n",
      "\u001b[1;31mSerialException\u001b[0m: could not open port 'COM3': OSError(22, 'Превышен таймаут семафора.', None, 121)"
     ]
    }
   ],
   "source": [
    "Servo1_ID = b'\\x01'\n",
    "Servo2_ID = b'\\x02'\n",
    "Servo3_ID = b'\\x03'\n",
    "\n",
    "com = 'COM3' #COM port id\n",
    "baudrate = 115200 #COM speed\n",
    "\n",
    "ser = serial.Serial(com, baudrate, timeout=0.2)\n",
    "Servo1 = Servo_lx16a(Servo1_ID, ser)\n",
    "Servo2 = Servo_lx16a(Servo2_ID, ser)\n",
    "Servo3 = Servo_lx16a(Servo3_ID, ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1963d-74f2-409a-8da5-66cd3f33f6aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da57424c-70bd-4b97-9d41-38b3583ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 12\n",
    "action_size = 3\n",
    "agent = DDPGAgent(state_size = state_size - 3, action_size = action_size, goal_size = 3, \n",
    "                  action_high = 5, action_low = -5, \n",
    "                  actor_learning_rate = 1e-3, critic_learning_rate = 1e-3,\n",
    "                  tau = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483f721d-155a-45fd-a6fb-f80773e4ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Servo():\n",
    "    def __init__(self):\n",
    "        self.angle = 0\n",
    "\n",
    "    def rotate(self, delta_angle):\n",
    "        self.angle += delta_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6503500-fbe0-49d4-b44a-f24ccf352bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_state(servo1, servo2, servo3):\n",
    "    # текущие углы роборуки\n",
    "    firstAngle = servo1.angle;\n",
    "    secondAngle = servo2.angle;\n",
    "    thirdAngle = servo3.angle;\n",
    "    \n",
    "    # длины двух сочленений роборуки\n",
    "    firstHandLength = 0.4048796\n",
    "    secondHandLength = 0.4048796\n",
    "    \n",
    "    # общая длина руки в вытянутом состоянии\n",
    "    allHandLength = firstHandLength + secondHandLength\n",
    "    \n",
    "    # позиция конца роборуки в вытянутом состоянии\n",
    "    defaultEndPosition = [0, allHandLength, 0]\n",
    "    \n",
    "    # поиск позиции конца роборуки по её углам и осям вращения\n",
    "    endPosition = get_rotate_offset_position(RotateAxis.OY, firstAngle, \n",
    "                                             RotateAxis.OX, secondAngle, \n",
    "                                             RotateAxis.OX, thirdAngle,\n",
    "                                             RotateAxis.OY, firstHandLength, \n",
    "                                             defaultEndPosition)\n",
    "    \n",
    "    cubePosition = [0, 0, 0.5]\n",
    "    \n",
    "    # векторное расстояние между концом руки и целью\n",
    "    vectorDistance = [cubePosition[0] - endPosition[0], \n",
    "                      cubePosition[1] - endPosition[1], \n",
    "                      cubePosition[2] - endPosition[2]]\n",
    "    \n",
    "    state = [vectorDistance[0], vectorDistance[1], vectorDistance[2], \n",
    "              endPosition[0], endPosition[1], endPosition[2],\n",
    "              firstAngle, secondAngle, thirdAngle]\n",
    "    \n",
    "    goal = [cubePosition[0], cubePosition[1], cubePosition[2]]\n",
    "    return state, goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8151e558-6a04-4f44-ac0b-93d43129d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def set_action(action):\n",
    "    #Servo1.MoveServo(action[0])\n",
    "    #Servo2.MoveServo(action[1])\n",
    "    #Servo3.MoveServo(action[2])\n",
    "    \n",
    "def set_action_test(action, servo1, servo2, servo3):\n",
    "    servo1.rotate(action[0])\n",
    "    servo2.rotate(action[1])    \n",
    "    servo3.rotate(action[2])\n",
    "    return servo1, servo2, servo3\n",
    "\n",
    "def check_if_done(hand, goal):\n",
    "    hand = np.array(hand)\n",
    "    goal = np.array(goal)\n",
    "    dist = np.linalg.norm(hand - goal)\n",
    "    if dist <= 0.5:\n",
    "        return True, dist\n",
    "    return False, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197ebc68-9429-4759-a0c6-b52caca0048c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " old state =  [0.0, -0.8097592, 0.5, 0.0, 0.8097592, 0.0, 0, 0, 0] \n",
      " action =  [0.51470989 5.         4.99822474] \n",
      " new state =  [-0.0009484692063728354, -0.8020696582055369, 0.3944224276088682, 0.0009484692063728354, 0.8020696582055369, 0.10557757239113182, 0.5147098898887634, 5.0, 4.99822473526001] \n",
      " distance =  0.8938040543702267 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -1 \n",
      "\n",
      "\n",
      " old state =  [-0.0009484692063728354, -0.8020696582055369, 0.3944224276088682, 0.0009484692063728354, 0.8020696582055369, 0.10557757239113182, 0.5147098898887634, 5.0, 4.99822473526001] \n",
      " action =  [ 4.99917459 -4.9980073   4.66259146] \n",
      " new state =  [-0.00653131215805904, -0.8040150226992169, 0.4323415879726325, 0.00653131215805904, 0.8040150226992169, 0.0676584120273675, 5.513884484767914, 0.0019927024841308594, 9.660816192626953] \n",
      " distance =  0.912908573437245 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -2 \n",
      "\n",
      "\n",
      " old state =  [-0.00653131215805904, -0.8040150226992169, 0.4323415879726325, 0.00653131215805904, 0.8040150226992169, 0.0676584120273675, 5.513884484767914, 0.0019927024841308594, 9.660816192626953] \n",
      " action =  [-4.99920368  4.99999857 -4.98126984] \n",
      " new state =  [-0.0009287350914362496, -0.8024508716139358, 0.39661326117505086, 0.0009287350914362496, 0.8024508716139358, 0.10338673882494914, 0.514680802822113, 5.001991271972656, 4.679546356201172] \n",
      " distance =  0.8951147093209588 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -3 \n",
      "\n",
      "\n",
      " old state =  [-0.0009287350914362496, -0.8024508716139358, 0.39661326117505086, 0.0009287350914362496, 0.8024508716139358, 0.10338673882494914, 0.514680802822113, 5.001991271972656, 4.679546356201172] \n",
      " action =  [ 4.99882936 -4.99955893  4.59998512] \n",
      " new state =  [-0.006276119683037653, -0.8044579243342782, 0.4349807070005058, 0.006276119683037653, 0.8044579243342782, 0.06501929299949423, 5.513510167598724, 0.0024323463439941406, 9.279531478881836] \n",
      " distance =  0.9145491551388317 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -4 \n",
      "\n",
      "\n",
      " old state =  [-0.006276119683037653, -0.8044579243342782, 0.4349807070005058, 0.006276119683037653, 0.8044579243342782, 0.06501929299949423, 5.513510167598724, 0.0024323463439941406, 9.279531478881836] \n",
      " action =  [-4.99877405  4.99999714 -4.98227406] \n",
      " new state =  [-0.0009049533153526782, -0.8028955194921115, 0.3992714731171349, 0.0009049533153526782, 0.8028955194921115, 0.10072852688286511, 0.5147361159324646, 5.002429485321045, 4.297257423400879] \n",
      " distance =  0.8966937846367273 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -5 \n",
      "\n",
      "\n",
      " old state =  [-0.0009049533153526782, -0.8028955194921115, 0.3992714731171349, 0.0009049533153526782, 0.8028955194921115, 0.10072852688286511, 0.5147361159324646, 5.002429485321045, 4.297257423400879] \n",
      " action =  [ 4.99822521 -4.999928    4.5068903 ] \n",
      " new state =  [-0.00595687336624197, -0.8049859204380672, 0.4382818505760082, 0.00595687336624197, 0.8049859204380672, 0.06171814942399178, 5.512961328029633, 0.0025014877319335938, 8.804147720336914] \n",
      " distance =  0.9165854008155235 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -6 \n",
      "\n",
      "\n",
      " old state =  [-0.00595687336624197, -0.8049859204380672, 0.4382818505760082, 0.00595687336624197, 0.8049859204380672, 0.06171814942399178, 5.512961328029633, 0.0025014877319335938, 8.804147720336914] \n",
      " action =  [-4.99823904  4.99999142 -4.97529125] \n",
      " new state =  [-0.0008755736609909775, -0.8034169361422944, 0.40253904473231666, 0.0008755736609909775, 0.8034169361422944, 0.09746095526768332, 0.5147222876548767, 5.002492904663086, 3.8288564682006836] \n",
      " distance =  0.8986196205533871 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -7 \n",
      "\n",
      "\n",
      " old state =  [-0.0008755736609909775, -0.8034169361422944, 0.40253904473231666, 0.0008755736609909775, 0.8034169361422944, 0.09746095526768332, 0.5147222876548767, 5.002492904663086, 3.8288564682006836] \n",
      " action =  [ 4.99204254 -4.99999237  4.37326002] \n",
      " new state =  [-0.005546438257455449, -0.8056151493449704, 0.4424692347499126, 0.005546438257455449, 0.8056151493449704, 0.05753076525008738, 5.506764829158783, 0.0025005340576171875, 8.2021164894104] \n",
      " distance =  0.919143925362963 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -8 \n",
      "\n",
      "\n",
      " old state =  [-0.005546438257455449, -0.8056151493449704, 0.4424692347499126, 0.005546438257455449, 0.8056151493449704, 0.05753076525008738, 5.506764829158783, 0.0025005340576171875, 8.2021164894104] \n",
      " action =  [-4.99709415  4.99995995 -4.90482569] \n",
      " new state =  [-0.0008339360915820199, -0.8039764545215712, 0.4062536643686177, 0.0008339360915820199, 0.8039764545215712, 0.09374633563138228, 0.5096706748008728, 5.002460479736328, 3.297290802001953] \n",
      " distance =  0.9007890289559539 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -9 \n",
      "\n",
      "\n",
      " old state =  [-0.0008339360915820199, -0.8039764545215712, 0.4062536643686177, 0.0008339360915820199, 0.8039764545215712, 0.09374633563138228, 0.5096706748008728, 5.002460479736328, 3.297290802001953] \n",
      " action =  [ 4.86413908 -5.          4.56516123] \n",
      " new state =  [-0.005190287767718404, -0.8059506784018877, 0.4448233015262981, 0.005190287767718404, 0.8059506784018877, 0.055176698473701886, 5.37380975484848, 0.002460479736328125, 7.862452030181885] \n",
      " distance =  0.9205711296170062 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -10 \n",
      "\n",
      "\n",
      " old state =  [-0.005190287767718404, -0.8059506784018877, 0.4448233015262981, 0.005190287767718404, 0.8059506784018877, 0.055176698473701886, 5.37380975484848, 0.002460479736328125, 7.862452030181885] \n",
      " action =  [-4.99556255  4.99989223 -4.80609941] \n",
      " new state =  [-0.0006077658862607419, -0.8042188560735888, 0.4079387389920953, 0.0006077658862607419, 0.8042188560735888, 0.0920612610079047, 0.3782472014427185, 5.002352714538574, 3.0563526153564453] \n",
      " distance =  0.901766240560238 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -11 \n",
      "\n",
      "\n",
      " old state =  [-0.0006077658862607419, -0.8042188560735888, 0.4079387389920953, 0.0006077658862607419, 0.8042188560735888, 0.0920612610079047, 0.3782472014427185, 5.002352714538574, 3.0563526153564453] \n",
      " action =  [ 4.35641956 -5.         -0.78204364] \n",
      " new state =  [-0.0013289497571886688, -0.8094396116373476, 0.4839545591787393, 0.0013289497571886688, 0.8094396116373476, 0.016045440821260696, 4.7346667647361755, 0.0023527145385742188, 2.274308979511261] \n",
      " distance =  0.9430833824985811 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -12 \n",
      "\n",
      "\n",
      " old state =  [-0.0013289497571886688, -0.8094396116373476, 0.4839545591787393, 0.0013289497571886688, 0.8094396116373476, 0.016045440821260696, 4.7346667647361755, 0.0023527145385742188, 2.274308979511261] \n",
      " action =  [ 5.         -4.49306774  4.9929595 ] \n",
      " new state =  [0.002043957595304579, -0.8080409230397685, 0.5119142351157234, -0.002043957595304579, 0.8080409230397685, -0.01191423511572343, 9.734666764736176, -4.490715026855469, 7.267268478870392] \n",
      " distance =  0.9565513552254937 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -13 \n",
      "\n",
      "\n",
      " old state =  [0.002043957595304579, -0.8080409230397685, 0.5119142351157234, -0.002043957595304579, 0.8080409230397685, -0.01191423511572343, 9.734666764736176, -4.490715026855469, 7.267268478870392] \n",
      " action =  [ 5.         -4.99991989  5.        ] \n",
      " new state =  [0.011991207105713643, -0.8037420987892799, 0.5455953505302011, -0.011991207105713643, 0.8037420987892799, -0.04559535053020113, 14.734666764736176, -9.49063491821289, 12.267268478870392] \n",
      " distance =  0.9715036988783017 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -14 \n",
      "\n",
      "\n",
      " old state =  [0.011991207105713643, -0.8037420987892799, 0.5455953505302011, -0.011991207105713643, 0.8037420987892799, -0.04559535053020113, 14.734666764736176, -9.49063491821289, 12.267268478870392] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.027585978296924128, -0.7964040555714745, 0.5768980134620348, -0.027585978296924128, 0.7964040555714745, -0.07689801346203487, 19.734666764736176, -14.49063491821289, 17.267268478870392] \n",
      " distance =  0.9837843889113776 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -15 \n",
      "\n",
      "\n",
      " old state =  [0.027585978296924128, -0.7964040555714745, 0.5768980134620348, -0.027585978296924128, 0.7964040555714745, -0.07689801346203487, 19.734666764736176, -14.49063491821289, 17.267268478870392] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.04831702341625622, -0.7860826572266407, 0.6048815402544516, -0.04831702341625622, 0.7860826572266407, -0.10488154025445168, 24.734666764736176, -19.49063491821289, 22.267268478870392] \n",
      " distance =  0.9930469054807538 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -16 \n",
      "\n",
      "\n",
      " old state =  [0.04831702341625622, -0.7860826572266407, 0.6048815402544516, -0.04831702341625622, 0.7860826572266407, -0.10488154025445168, 24.734666764736176, -19.49063491821289, 22.267268478870392] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.07351837052824664, -0.7728564558284132, 0.6287104108101709, -0.07351837052824664, 0.7728564558284132, -0.12871041081017087, 29.734666764736176, -24.49063491821289, 27.267268478870392] \n",
      " distance =  0.9989939102826796 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -17 \n",
      "\n",
      "\n",
      " old state =  [0.07351837052824664, -0.7728564558284132, 0.6287104108101709, -0.07351837052824664, 0.7728564558284132, -0.12871041081017087, 29.734666764736176, -24.49063491821289, 27.267268478870392] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.10238964221846342, -0.7568261107556311, 0.6476784297238686, -0.10238964221846342, 0.7568261107556311, -0.14767842972386858, 34.734666764736176, -29.49063491821289, 32.26726847887039] \n",
      " distance =  1.0013774258913049 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -18 \n",
      "\n",
      "\n",
      " old state =  [0.10238964221846342, -0.7568261107556311, 0.6476784297238686, -0.10238964221846342, 0.7568261107556311, -0.14767842972386858, 34.734666764736176, -29.49063491821289, 32.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.13402063834261177, -0.7381136226136856, 0.6612300479540809, -0.13402063834261177, 0.7381136226136856, -0.16123004795408097, 39.734666764736176, -34.49063491821289, 37.26726847887039] \n",
      " distance =  0.9999992138531988 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -19 \n",
      "\n",
      "\n",
      " old state =  [0.13402063834261177, -0.7381136226136856, 0.6612300479540809, -0.13402063834261177, 0.7381136226136856, -0.16123004795408097, 39.734666764736176, -34.49063491821289, 37.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.167419242851692, -0.7168614047362459, 0.6689770847162374, -0.167419242851692, 0.7168614047362459, -0.16897708471623743, 44.734666764736176, -39.49063491821289, 42.26726847887039] \n",
      " distance =  0.9947109209981027 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -20 \n",
      "\n",
      "\n",
      " old state =  [0.167419242851692, -0.7168614047362459, 0.6689770847162374, -0.167419242851692, 0.7168614047362459, -0.16897708471623743, 44.734666764736176, -39.49063491821289, 42.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.20154180484929585, -0.6932311993337996, 0.670710342714439, -0.20154180484929585, 0.6932311993337996, -0.170710342714439, 49.734666764736176, -44.49063491821289, 47.26726847887039] \n",
      " distance =  0.9854141051638189 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -21 \n",
      "\n",
      "\n",
      " old state =  [0.20154180484929585, -0.6932311993337996, 0.670710342714439, -0.20154180484929585, 0.6932311993337996, -0.170710342714439, 49.734666764736176, -44.49063491821289, 47.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.2353250689710261, -0.667402846537767, 0.6664057651547259, -0.2353250689710261, 0.667402846537767, -0.16640576515472597, 54.734666764736176, -49.49063491821289, 52.26726847887039] \n",
      " distance =  0.9720602303789556 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -22 \n",
      "\n",
      "\n",
      " old state =  [0.2353250689710261, -0.667402846537767, 0.6664057651547259, -0.2353250689710261, 0.667402846537767, -0.16640576515472597, 54.734666764736176, -49.49063491821289, 52.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.2677186832265178, -0.6395729157085123, 0.656224948979205, -0.2677186832265178, 0.6395729157085123, -0.15622494897920502, 59.734666764736176, -54.49063491821289, 57.26726847887039] \n",
      " distance =  0.9546507170264885 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -23 \n",
      "\n",
      "\n",
      " old state =  [0.2677186832265178, -0.6395729157085123, 0.656224948979205, -0.2677186832265178, 0.6395729157085123, -0.15622494897920502, 59.734666764736176, -54.49063491821289, 57.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.29771729503721756, -0.6099532094238174, 0.640510000383653, -0.29771729503721756, 0.6099532094238174, -0.140510000383653, 64.73466676473618, -59.49063491821289, 62.26726847887039] \n",
      " distance =  0.9332371435182808 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -24 \n",
      "\n",
      "\n",
      " old state =  [0.29771729503721756, -0.6099532094238174, 0.640510000383653, -0.29771729503721756, 0.6099532094238174, -0.140510000383653, 64.73466676473618, -59.49063491821289, 62.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.32439125886578307, -0.5787691515333766, 0.6197728907153286, -0.32439125886578307, 0.5787691515333766, -0.11977289071532864, 69.73466676473618, -64.49063491821289, 67.26726847887039] \n",
      " distance =  0.9079217233114465 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -25 \n",
      "\n",
      "\n",
      " old state =  [0.32439125886578307, -0.5787691515333766, 0.6197728907153286, -0.32439125886578307, 0.5787691515333766, -0.11977289071532864, 69.73466676473618, -64.49063491821289, 67.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.34691502117982426, -0.5462580715471851, 0.594679638064575, -0.34691502117982426, 0.5462580715471851, -0.09467963806457506, 74.73466676473618, -69.49063491821289, 72.26726847887039] \n",
      " distance =  0.8788582278042696 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -26 \n",
      "\n",
      "\n",
      " old state =  [0.34691502117982426, -0.5462580715471851, 0.594679638064575, -0.34691502117982426, 0.5462580715471851, -0.09467963806457506, 74.73466676473618, -69.49063491821289, 72.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.36459231923578284, -0.5126673984146646, 0.5660297971817051, -0.36459231923578284, 0.5126673984146646, -0.06602979718170511, 79.73466676473618, -74.49063491821289, 77.26726847887039] \n",
      " distance =  0.846253597889279 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -27 \n",
      "\n",
      "\n",
      " old state =  [0.36459231923578284, -0.5126673984146646, 0.5660297971817051, -0.36459231923578284, 0.5126673984146646, -0.06602979718170511, 79.73466676473618, -74.49063491821289, 77.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3768774271572806, -0.47825277744095673, 0.5347318829888659, -0.3768774271572806, 0.47825277744095673, -0.034731882988865904, 84.73466676473618, -79.49063491821289, 82.26726847887039] \n",
      " distance =  0.8103705947993787 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -28 \n",
      "\n",
      "\n",
      " old state =  [0.3768774271572806, -0.47825277744095673, 0.5347318829888659, -0.3768774271572806, 0.47825277744095673, -0.034731882988865904, 84.73466676473618, -79.49063491821289, 82.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3833918030679359, -0.4432761246717874, 0.5017754765804399, -0.3833918030679359, 0.4432761246717874, -0.0017754765804398494, 89.73466676473618, -84.49063491821289, 87.26726847887039] \n",
      " distance =  0.7715319995056903 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -29 \n",
      "\n",
      "\n",
      " old state =  [0.3833918030679359, -0.4432761246717874, 0.5017754765804399, -0.3833918030679359, 0.4432761246717874, -0.0017754765804398494, 89.73466676473618, -84.49063491821289, 87.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3839356309652308, -0.40800363355421065, 0.46820086345828227, -0.3839356309652308, 0.40800363355421065, 0.03179913654171776, 94.73466676473618, -89.49063491821289, 92.26726847887039] \n",
      " distance =  0.7301271000731239 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -30 \n",
      "\n",
      "\n",
      " old state =  [0.3839356309652308, -0.40800363355421065, 0.46820086345828227, -0.3839356309652308, 0.40800363355421065, 0.03179913654171776, 94.73466676473618, -89.49063491821289, 92.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.37849390633647134, -0.3727037490437435, 0.43506712876498776, -0.37849390633647134, 0.3727037490437435, 0.06493287123501221, 99.73466676473618, -94.49063491821289, 97.26726847887039] \n",
      " distance =  0.6866215320079276 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -31 \n",
      "\n",
      "\n",
      " old state =  [0.37849390633647134, -0.3727037490437435, 0.43506712876498776, -0.37849390633647134, 0.3727037490437435, 0.06493287123501221, 99.73466676473618, -94.49063491821289, 97.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3672368804995875, -0.3376451245761637, 0.4034196811797745, -0.3672368804995875, 0.3376451245761637, 0.0965803188202255, 104.73466676473618, -99.49063491821289, 102.26726847887039] \n",
      " distance =  0.6415719723556448 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -32 \n",
      "\n",
      "\n",
      " old state =  [0.3672368804995875, -0.3376451245761637, 0.4034196811797745, -0.3672368804995875, 0.3376451245761637, 0.0965803188202255, 104.73466676473618, -99.49063491821289, 102.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3505148502557121, -0.30309457745264584, 0.37425819450228726, -0.3505148502557121, 0.30309457745264584, 0.12574180549771272, 109.73466676473618, -104.49063491821289, 107.26726847887039] \n",
      " distance =  0.5956476972868224 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -33 \n",
      "\n",
      "\n",
      " old state =  [0.3505148502557121, -0.30309457745264584, 0.37425819450228726, -0.3505148502557121, 0.30309457745264584, 0.12574180549771272, 109.73466676473618, -104.49063491821289, 107.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3288474514515406, -0.26931505819899043, 0.34850594324213446, -0.3288474514515406, 0.26931505819899043, 0.15149405675786554, 114.73466676473618, -109.49063491821289, 112.26726847887039] \n",
      " distance =  0.5496613861041985 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -34 \n",
      "\n",
      "\n",
      " old state =  [0.3288474514515406, -0.26931505819899043, 0.34850594324213446, -0.3288474514515406, 0.26931505819899043, 0.15149405675786554, 114.73466676473618, -109.49063491821289, 112.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.3029077822394636, -0.23656364935334956, 0.32698146614251533, -0.3029077822394636, 0.23656364935334956, 0.1730185338574847, 119.73466676473618, -114.49063491821289, 117.26726847887039] \n",
      " distance =  0.5046111016786231 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -35 \n",
      "\n",
      "\n",
      " old state =  [0.3029077822394636, -0.23656364935334956, 0.32698146614251533, -0.3029077822394636, 0.23656364935334956, 0.1730185338574847, 119.73466676473618, -114.49063491821289, 117.26726847887039] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [0.2735018391109057, -0.20508960891287953, 0.3103734207831215, -0.2735018391109057, 0.20508960891287953, 0.18962657921687848, 124.73466676473618, -119.49063491821289, 122.26726847887039] \n",
      " distance =  0.46173224276598035 \n",
      " current reward =  9 \n",
      " cumulative reward =  -26 \n",
      "\n",
      "episode: 1 rewards: -26.00\r"
     ]
    }
   ],
   "source": [
    "n_episodes = 1\n",
    "\n",
    "servo1 = Servo()\n",
    "servo2 = Servo()\n",
    "servo3 = Servo()\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/without_her_rotate.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    state, goal = read_state(servo1, servo2, servo3)\n",
    "    \n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action([state], [goal], 0)\n",
    "        \n",
    "        # применяем углы к сервам\n",
    "        servo1, servo2, servo3 = set_action_test(action, servo1, servo2, servo3)\n",
    "        \n",
    "        # как-то ждем, пока сервы повернутся\n",
    "        time.sleep(.1)\n",
    "        \n",
    "        # получаем обновленный state\n",
    "        next_state, next_goal = read_state(servo1, servo2, servo3)\n",
    "        \n",
    "        done, distance = check_if_done(next_state[3:6], goal)\n",
    "        \n",
    "        reward = -1\n",
    "        \n",
    "        if done:\n",
    "            reward += 10\n",
    "        \n",
    "        r += reward\n",
    "        \n",
    "        print(\"\\n old state = \", state, \"\\n action = \", action, \"\\n new state = \", next_state, \"\\n distance = \", distance,\n",
    "              \"\\n current reward = \", reward, \"\\n cumulative reward = \", r, \"\\n\")\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ea901-3e3f-47b4-b118-6da592abb883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
