{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a1970a-70fc-4997-934e-e02558918bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\vika9\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q pyserial\n",
    "\n",
    "from MathTools import get_rotate_offset_position\n",
    "from MathTools import RotateAxis\n",
    "from MathTools import normalize_coordinates\n",
    "\n",
    "from DDPGAgent import DDPGAgent\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import serial, struct, time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af473b24-c39a-4eda-b10d-bb332c140e85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7206709d-3323-4a62-a7c1-822e7fa4a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Servo_lx16a:\n",
    "    \n",
    "    def __init__(self, ID, Port, Interlock = False):\n",
    "        self.ID = ID # unique servo ID\n",
    "        self.Port = Port # COM port features for later use \n",
    "        self.Interlock = Interlock # While Interlock = True, Servo will not move\n",
    "        \n",
    "    @staticmethod   \n",
    "    def checksum(msg):\n",
    "        #Checksum=~(ID+Length+Cmd+Prm1+...+PrmN)\n",
    "        #If the numbers in the brackets exceeds 255,\n",
    "        #Take the lowest one byte, \"~\" means Negation.\n",
    "        s = sum(msg)\n",
    "        s = ~s&255\n",
    "        chksum = s.to_bytes(1, byteorder ='little')\n",
    "        return chksum\n",
    "    \n",
    "    @staticmethod  \n",
    "    def display_msg(msg):\n",
    "        #dispalys bytes in python console not as ASCII symbols but as bytes\n",
    "        #print(''.join(r'\\x'+hex(letter)[2:] for letter in msg))\n",
    "        pass\n",
    "      \n",
    "    @staticmethod     \n",
    "    def time(current_position, target_position):\n",
    "        #currently not used\n",
    "        #range of time for Servo movement is 0~30000ms\n",
    "        #3000ms for 1000 steps\n",
    "        #y ms for N steps\n",
    "        #y = N*3000/1000\n",
    "        return abs(target_position-current_position)*3000/1000\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_angle(position):\n",
    "        #returns Servo angle from Servo position\n",
    "        #Range of position is 0..1000 which is equal to 0..240 deg\n",
    "        #240 degree equals to 1000 \n",
    "        #x degree equals to N\n",
    "        #x= (N*240/1000)\n",
    "        return position*240/1000\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_position(angle):\n",
    "        #return position from angle\n",
    "        return int(angle*1000/240)\n",
    "    \n",
    "    Header = b'\\x55\\x55' #every message transfered and recieved starts with this Header\n",
    "    \n",
    "    def ReadPosition(self):\n",
    "        #Reads current position of Servo \n",
    "        temp = self.ID+b'\\x03'+b'\\x1c' \n",
    "        msg = self.Header + temp + self.checksum(temp) #message to read Servo position\n",
    "        #print(f'Reading position of servo {int.from_bytes(self.ID, \"little\")}, sent:')\n",
    "        self.display_msg(msg)\n",
    "\n",
    "        wait = True\n",
    "        data_in = b'' # receive answer in data_in while wait True\n",
    "        ser.write(msg) #send message to COM\n",
    "        while wait:\n",
    "            while ser.in_waiting:\n",
    "                data_in += ser.readline()\n",
    "                #print('Received: ')\n",
    "                self.display_msg(data_in)\n",
    "                if len(data_in)>=8: wait = False\n",
    "        received_chksum = self.checksum(data_in[-6:-1]) \n",
    "        if data_in.startswith(self.Header): #check that received data_in startswith Header and that checksum is correct\n",
    "            #print('Message Header correct')\n",
    "            if  (received_chksum == data_in[-1].to_bytes(1, 'big')):\n",
    "                #print('Checksum correct')\n",
    "                position = struct.unpack('<h', data_in[-3:-1])[0]\n",
    "                angle = self.to_angle(position)\n",
    "                #print(f'Current position {position}, {angle}')\n",
    "                #print('---------------------------------------')\n",
    "                return angle\n",
    "            else:\n",
    "                print('Bad checksum')\n",
    "        else:\n",
    "            print('Bad message header')\n",
    "\n",
    "\n",
    "\n",
    "    def MoveServo(self, target_angle):\n",
    "        #Moves servo to traget angle\n",
    "        if self.Interlock:\n",
    "            print('Servo is Interlocked') #if Servo is interlocked, it will not move\n",
    "        else:\n",
    "            target_position = self.to_position(target_angle) \n",
    "            temp = self.ID+b'\\x07'+b'\\x01'+struct.pack('<h', target_position)+struct.pack('<h', 1500) #set time temporary to 1500 ms for any movement\n",
    "            msg = self.Header+temp+self.checksum(temp) # message to move servo to desired position\n",
    "            #print(f'Move servo {int.from_bytes(self.ID, \"little\")} to {target_position}, sent:')\n",
    "            self.display_msg(msg)\n",
    "            self.Interlock = True #Interlock servo\n",
    "            ser.write(msg) #sending message to COM\n",
    "            #This while cycle switches Interlock to False, when Servo reaches target position\n",
    "            while self.Interlock:\n",
    "                current_angle = self.ReadPosition()\n",
    "                angle_diff = abs(current_angle-target_angle)\n",
    "                #print('Angle_difference:', angle_diff)\n",
    "                #print('---------------------------------------')\n",
    "                if angle_diff<0.8:\n",
    "                    self.Interlock=False\n",
    "                    #print('Reached target angle +- 1 deg')\n",
    "                    #print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd80fec-4571-47e2-b763-4928456b92ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0151087040722454, 0.8343491981978997, 0.07299609626426051]\n"
     ]
    }
   ],
   "source": [
    "test_result = get_rotate_offset_position(RotateAxis.OY, 0, RotateAxis.OX, 5, RotateAxis.OZ, 2,\n",
    "                                             RotateAxis.OY, 0.4048796, [0, 0.8378, 0])\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099530fa-5b1e-44ee-b1ba-36433f457787",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Servo settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4bca50-c553-4f24-af51-6167cc218e2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mser\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ser' is not defined"
     ]
    }
   ],
   "source": [
    "ser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9c7207-fba6-4198-a4d9-339f04649186",
   "metadata": {},
   "outputs": [],
   "source": [
    "Servo1_ID = b'\\x01'\n",
    "Servo2_ID = b'\\x02'\n",
    "Servo3_ID = b'\\x03'\n",
    "\n",
    "com = 'COM9' #COM port id|\n",
    "baudrate = 115200 #COM speed\n",
    "\n",
    "ser = serial.Serial(com, baudrate, timeout=0.2)\n",
    "Servo1 = Servo_lx16a(Servo1_ID, ser)\n",
    "Servo2 = Servo_lx16a(Servo2_ID, ser) # 0 - 110\n",
    "Servo3 = Servo_lx16a(Servo3_ID, ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1963d-74f2-409a-8da5-66cd3f33f6aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da57424c-70bd-4b97-9d41-38b3583ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 12\n",
    "action_size = 3\n",
    "agent = DDPGAgent(state_size = state_size - 3, action_size = action_size, goal_size = 3, \n",
    "                  action_high = 5, action_low = -5, \n",
    "                  actor_learning_rate = 1e-3, critic_learning_rate = 1e-3,\n",
    "                  tau = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483f721d-155a-45fd-a6fb-f80773e4ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Servo():\n",
    "    def __init__(self):\n",
    "        self.angle = 0\n",
    "\n",
    "    def rotate(self, delta_angle):\n",
    "        self.angle += delta_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6503500-fbe0-49d4-b44a-f24ccf352bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_state_test(servo1, servo2, servo3):\n",
    "    # текущие углы роборуки\n",
    "    firstAngle = servo1.angle;\n",
    "    secondAngle = servo2.angle;\n",
    "    thirdAngle = servo3.angle;\n",
    "        \n",
    "    # длины двух сочленений роборуки\n",
    "    firstHandLength = 12 #0.4048796\n",
    "    secondHandLength = 9 #0.4048796\n",
    "    \n",
    "    # общая длина руки в вытянутом состоянии\n",
    "    allHandLength = firstHandLength + secondHandLength\n",
    "    \n",
    "    # позиция конца роборуки в вытянутом состоянии\n",
    "    defaultEndPosition = [0, allHandLength, 0]\n",
    "    \n",
    "    # не надо нормализовать, тк мы находим позицию для настоящих углов и длин, после чего нормализуем найденную позицию\n",
    "    #defaultEndPosition = normalize_coordinates(allHandLength, 1, defaultEndPosition)\n",
    "    \n",
    "    # поиск позиции конца роборуки по её углам и осям вращения\n",
    "    endPosition = get_rotate_offset_position(RotateAxis.OY, firstAngle, \n",
    "                                             RotateAxis.OX, secondAngle, \n",
    "                                             RotateAxis.OX, thirdAngle,\n",
    "                                             RotateAxis.OY, firstHandLength, \n",
    "                                             defaultEndPosition)\n",
    "    \n",
    "    # не надо нормализовать, из-за этого находится неправильная позиция\n",
    "    #endPosition = normalize_coordinates(allHandLength, 1, endPosition)\n",
    "    \n",
    "    cubePosition = [0, 12, 0]\n",
    "    #cubePosition = normalize_coordinates(allHandLength, 1, cubePosition)\n",
    "    \n",
    "    print(f'defaultEndPosition = {defaultEndPosition}; rotated endPosition = {endPosition}; cubePosition = {cubePosition}')\n",
    "    \n",
    "    # векторное расстояние между концом руки и целью\n",
    "    vectorDistance = [cubePosition[0] - endPosition[0], \n",
    "                      cubePosition[1] - endPosition[1], \n",
    "                      cubePosition[2] - endPosition[2]]\n",
    "    \n",
    "    firstAngle = firstAngle/360\n",
    "    secondAngle = secondAngle/360\n",
    "    thirdAngle = thirdAngle/360\n",
    "    \n",
    "    state = [vectorDistance[0], vectorDistance[1], vectorDistance[2], \n",
    "              endPosition[0], endPosition[1], endPosition[2],\n",
    "              firstAngle, secondAngle, thirdAngle]\n",
    "    \n",
    "    goal = [cubePosition[0], cubePosition[1], cubePosition[2]]\n",
    "    return state, goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebf61a1-fbed-40c5-b4a7-c525ac0d2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_state():\n",
    "    firstAngle = Servo1.ReadPosition() - 120 #Servo1.ReadPosition() + 120;\n",
    "    secondAngle = Servo2.ReadPosition() - 120 #Servo2.ReadPosition() + 120;\n",
    "    thirdAngle = Servo3.ReadPosition() - 120 #240 - Servo3.ReadPosition() + 120;\n",
    "    \n",
    "    print(\"\\n firstAngle = \", firstAngle, \" secondAngle = \", secondAngle, \" thirdAngle = \", thirdAngle, \"\\n\")\n",
    "    \n",
    "    # длины двух сочленений роборуки\n",
    "    firstHandLength = 12\n",
    "    secondHandLength = 9\n",
    "    \n",
    "    # общая длина руки в вытянутом состоянии\n",
    "    allHandLength = firstHandLength + secondHandLength\n",
    "    \n",
    "    # позиция конца роборуки в вытянутом состоянии\n",
    "    defaultEndPosition = [0, allHandLength, 0]\n",
    "    \n",
    "    # надо ли нормализовать?????\n",
    "    defaultEndPosition = normalize_coordinates(allHandLength, 1, defaultEndPosition)\n",
    "    \n",
    "    # поиск позиции конца роборуки по её углам и осям вращения\n",
    "    endPosition = get_rotate_offset_position(RotateAxis.OY, firstAngle, \n",
    "                                             RotateAxis.OX, secondAngle, \n",
    "                                             RotateAxis.OX, thirdAngle,\n",
    "                                             RotateAxis.OY, firstHandLength, \n",
    "                                             defaultEndPosition)\n",
    "    \n",
    "    print(f'not normalize end pos: {endPosition}')\n",
    "    \n",
    "    #not normalize end pos: [-0.0007017679341761611, 1.0011229079971835, 0.08376541700570701]\n",
    "    #normalize end pos: [-3.341752067505529e-05, 0.04767251942843731, 0.0039888293812241436]\n",
    "    \n",
    "    #endPosition = normalize_coordinates(allHandLength, 1, endPosition)\n",
    "    \n",
    "    cubePosition = [-0.007918067592270591, 0.7898086317810685, 0.6300666310496583] #endPosition #[-10, 6, 0]\n",
    "    \n",
    "    #cubePosition = normalize_coordinates(allHandLength, 1, cubePosition)\n",
    "    \n",
    "    # векторное расстояние между концом руки и целью\n",
    "    vectorDistance = [cubePosition[0] - endPosition[0], \n",
    "                      cubePosition[1] - endPosition[1], \n",
    "                      cubePosition[2] - endPosition[2]]\n",
    "    \n",
    "    firstAngle = firstAngle/360\n",
    "    secondAngle = secondAngle/360\n",
    "    thirdAngle = thirdAngle/360\n",
    "    \n",
    "    state = [vectorDistance[0], vectorDistance[1], vectorDistance[2], \n",
    "              endPosition[0], endPosition[1], endPosition[2],\n",
    "              firstAngle, secondAngle, thirdAngle]\n",
    "    \n",
    "    goal = [cubePosition[0], cubePosition[1], cubePosition[2]]\n",
    "    return state, goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a28e612e-6290-4820-8a1e-43375cfb0c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " firstAngle =  -40.31999999999999  secondAngle =  -0.23999999999999488  thirdAngle =  -80.4 \n",
      "\n",
      "not normalize end pos: [-6.99032676138843, 10.210885907229931, 8.236879969108381]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Servo2.MoveServo(150) #end pos: [-0.007918067592270591, 0.7898086317810685, 0.6300666310496583] - повернулся в положительную сторону по z\n",
    "#Servo2.MoveServo(80) #end pos: [0.002591194524373274, 0.7871754797239352, -0.6185985076056891] - повернулся в отрицательную сторону по z\n",
    "\n",
    "#Servo3.MoveServo(150) #end pos: [0.04649725024516806, 2.4736152827448286, -5.550070567099335]\n",
    "#Servo3.MoveServo(80) #end pos: [-0.05975396614459472, 3.583286569751135, 7.132437445610642]\n",
    "\n",
    "#Servo3.MoveServo(40) #end pos: [-0.09056626981047014, 10.256364693800629, 10.81029922837861]\n",
    "#Servo1.MoveServo(150) # против часовой, end pos: [5.444508565215012, 10.256364693800629, 9.339598392843824]\n",
    "Servo1.MoveServo(80) # по часовой, end pos: [-6.99032676138843, 10.210885907229931, 8.236879969108381]\n",
    "\n",
    "state, goal = read_state()\n",
    "\n",
    "done, distance = check_if_done(state[3:6], goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8151e558-6a04-4f44-ac0b-93d43129d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_action(action):\n",
    "    angle1 = Servo1.ReadPosition() + action[0]\n",
    "    angle2 = Servo2.ReadPosition() + action[1]\n",
    "    \n",
    "    # так как 3 серва крутится в другую сторону, относительно 1 и 2 серв, то поворачиваем на -action[2]\n",
    "    angle3 = Servo3.ReadPosition() - action[2]\n",
    "    \n",
    "    if (angle1 >=0 and angle1 <= 240):\n",
    "        Servo1.MoveServo(angle1)\n",
    "    if angle2 >=0 and angle1 <= 240:\n",
    "        Servo2.MoveServo(angle2)\n",
    "    if angle3 >=0 and angle1 <= 240:    \n",
    "        Servo3.MoveServo(angle3)\n",
    "    \n",
    "def set_action_test(action, servo1, servo2, servo3):\n",
    "    servo1.rotate(action[0])\n",
    "    servo2.rotate(action[1])    \n",
    "    servo3.rotate(action[2])\n",
    "    \n",
    "    return servo1, servo2, servo3\n",
    "\n",
    "def check_if_done(hand, goal):\n",
    "    hand = np.array(hand)\n",
    "    goal = np.array(goal)\n",
    "    dist = np.linalg.norm(hand - goal)\n",
    "    if dist <= 0.05:\n",
    "        return True, dist\n",
    "    return False, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e6b71a-4a05-4a21-89e8-1c2d91db110a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Servo1.MoveServo(120)\n",
    "Servo2.MoveServo(120)\n",
    "Servo3.MoveServo(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8edd0-3b1a-4bab-9ed0-aff48c1e81fa",
   "metadata": {},
   "source": [
    "## Test actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8813074b-3bb6-4171-9f5f-4faeb78e3abb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate_scaled.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate_scaled.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [0.0, 1.0, 0.0]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.47619047619047616, -1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.01082682881137139, 0.9913145787719438, 0.12375121958701844]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.47619047619047616, -1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.4653636473791048, -0.9913145787719438, 0.16196306612726724, -0.01082682881137139, 0.9913145787719438, 0.12375121958701844, -0.013888888888888888, 0.013888888888888888, 0.013888888888888888] \n",
      " distance =  1.1070230138425876 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -1 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.4653636473791048, -0.9913145787719438, 0.16196306612726724, -0.01082682881137139, 0.9913145787719438, 0.12375121958701844, -0.013888888888888888, 0.013888888888888888, 0.013888888888888888] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.04268404032426247, 0.9654726963437938, 0.242073221878819]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.4653636473791048, -0.9913145787719438, 0.16196306612726724, -0.01082682881137139, 0.9913145787719438, 0.12375121958701844, -0.013888888888888888, 0.013888888888888888, 0.013888888888888888] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.4335064358662137, -0.9654726963437938, 0.043641063835466704, -0.04268404032426247, 0.9654726963437938, 0.242073221878819, -0.027777777777777776, 0.027777777777777776, 0.027777777777777776] \n",
      " distance =  1.059230805714918 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -2 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.4335064358662137, -0.9654726963437938, 0.043641063835466704, -0.04268404032426247, 0.9654726963437938, 0.242073221878819, -0.027777777777777776, 0.027777777777777776, 0.027777777777777776] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.09373968001212911, 0.9231113595013699, 0.3498412484905146]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.4335064358662137, -0.9654726963437938, 0.043641063835466704, -0.04268404032426247, 0.9654726963437938, 0.242073221878819, -0.027777777777777776, 0.027777777777777776, 0.027777777777777776] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.38245079617834704, -0.9231113595013699, -0.06412696277622892, -0.09373968001212911, 0.9231113595013699, 0.3498412484905146, -0.041666666666666664, 0.041666666666666664, 0.041666666666666664] \n",
      " distance =  1.0012569404967053 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -3 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.38245079617834704, -0.9231113595013699, -0.06412696277622892, -0.09373968001212911, 0.9231113595013699, 0.3498412484905146, -0.041666666666666664, 0.041666666666666664, 0.041666666666666664] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.1610642921344152, 0.8652719732143669, 0.4425205057197485]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.38245079617834704, -0.9231113595013699, -0.06412696277622892, -0.09373968001212911, 0.9231113595013699, 0.3498412484905146, -0.041666666666666664, 0.041666666666666664, 0.041666666666666664] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.31512618405606097, -0.8652719732143669, -0.1568062200054628, -0.1610642921344152, 0.8652719732143669, 0.4425205057197485, -0.05555555555555555, 0.05555555555555555, 0.05555555555555555] \n",
      " distance =  0.9341243440465622 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -4 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.31512618405606097, -0.8652719732143669, -0.1568062200054628, -0.1610642921344152, 0.8652719732143669, 0.4425205057197485, -0.05555555555555555, 0.05555555555555555, 0.05555555555555555] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.24080827050401646, 0.7933705681723169, 0.5164150026118011]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.31512618405606097, -0.8652719732143669, -0.1568062200054628, -0.1610642921344152, 0.8652719732143669, 0.4425205057197485, -0.05555555555555555, 0.05555555555555555, 0.05555555555555555] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.2353822056864597, -0.7933705681723169, -0.23070071689751537, -0.24080827050401646, 0.7933705681723169, 0.5164150026118011, -0.06944444444444445, 0.06944444444444445, 0.06944444444444445] \n",
      " distance =  0.8591067814730106 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -5 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.2353822056864597, -0.7933705681723169, -0.23070071689751537, -0.24080827050401646, 0.7933705681723169, 0.5164150026118011, -0.06944444444444445, 0.06944444444444445, 0.06944444444444445] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.32843401509666537, 0.7091573735911079, 0.5688644010812682]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.2353822056864597, -0.7933705681723169, -0.23070071689751537, -0.24080827050401646, 0.7933705681723169, 0.5164150026118011, -0.06944444444444445, 0.06944444444444445, 0.06944444444444445] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-0.1477564610938108, -0.7091573735911079, -0.28315011536698254, -0.32843401509666537, 0.7091573735911079, 0.5688644010812682, -0.08333333333333333, 0.08333333333333333, 0.08333333333333333] \n",
      " distance =  0.777759693058171 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -6 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-4.99998236  5.          5.        ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.1477564610938108, -0.7091573735911079, -0.28315011536698254, -0.32843401509666537, 0.7091573735911079, 0.5688644010812682, -0.08333333333333333, 0.08333333333333333, 0.08333333333333333] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4189878653766672, 0.6146669438761391, 0.5983770769512654]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.1477564610938108, -0.7091573735911079, -0.28315011536698254, -0.32843401509666537, 0.7091573735911079, 0.5688644010812682, -0.08333333333333333, 0.08333333333333333, 0.08333333333333333] \n",
      " action =  [-4.99998236  5.          5.        ] \n",
      " new state =  [-0.05720261081380895, -0.6146669438761391, -0.3126627912369797, -0.4189878653766672, 0.6146669438761391, 0.5983770769512654, -0.09722217321395873, 0.09722222222222222, 0.09722222222222222] \n",
      " distance =  0.6919867134577427 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -7 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-2.83469868  5.          5.        ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.05720261081380895, -0.6146669438761391, -0.3126627912369797, -0.4189878653766672, 0.6146669438761391, 0.5983770769512654, -0.09722217321395873, 0.09722222222222222, 0.09722222222222222] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.48418646321849335, 0.5121603293538147, 0.62342985895165]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.05720261081380895, -0.6146669438761391, -0.3126627912369797, -0.4189878653766672, 0.6146669438761391, 0.5983770769512654, -0.09722217321395873, 0.09722222222222222, 0.09722222222222222] \n",
      " action =  [-2.83469868  5.          5.        ] \n",
      " new state =  [0.007995987028017182, -0.5121603293538147, -0.33771557323736434, -0.48418646321849335, 0.5121603293538147, 0.62342985895165, -0.10509633620580038, 0.1111111111111111, 0.1111111111111111] \n",
      " distance =  0.6135339820901543 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -8 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [3.51703382 4.99997997 5.        ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.007995987028017182, -0.5121603293538147, -0.33771557323736434, -0.48418646321849335, 0.5121603293538147, 0.62342985895165, -0.10509633620580038, 0.1111111111111111, 0.1111111111111111] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.46942182583473785, 0.404061308859116, 0.6876915769592761]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.007995987028017182, -0.5121603293538147, -0.33771557323736434, -0.48418646321849335, 0.5121603293538147, 0.62342985895165, -0.10509633620580038, 0.1111111111111111, 0.1111111111111111] \n",
      " action =  [3.51703382 4.99997997 5.        ] \n",
      " new state =  [-0.006768650355738315, -0.404061308859116, -0.4019772912449904, -0.46942182583473785, 0.404061308859116, 0.6876915769592761, -0.09532679782973395, 0.12499994436899821, 0.125] \n",
      " distance =  0.5699974549252304 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -9 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [0.51604259 4.20572567 5.        ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.006768650355738315, -0.404061308859116, -0.4019772912449904, -0.46942182583473785, 0.404061308859116, 0.6876915769592761, -0.09532679782973395, 0.12499994436899821, 0.125] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4760183493701761, 0.3047774864948283, 0.7110246437522478]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.006768650355738315, -0.404061308859116, -0.4019772912449904, -0.46942182583473785, 0.404061308859116, 0.6876915769592761, -0.09532679782973395, 0.12499994436899821, 0.125] \n",
      " action =  [0.51604259 4.20572567 5.        ] \n",
      " new state =  [-0.00017212682030004478, -0.3047774864948283, -0.42531035803796213, -0.4760183493701761, 0.3047774864948283, 0.7110246437522478, -0.09389334619045257, 0.1366825156741672, 0.1388888888888889] \n",
      " distance =  0.5232382311682959 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -10 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.85779995 -0.42356807  5.        ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.00017212682030004478, -0.3047774864948283, -0.42531035803796213, -0.4760183493701761, 0.3047774864948283, 0.7110246437522478, -0.09389334619045257, 0.1366825156741672, 0.1388888888888889] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4811552399559759, 0.2744291339355313, 0.695929424970223]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.00017212682030004478, -0.3047774864948283, -0.42531035803796213, -0.4760183493701761, 0.3047774864948283, 0.7110246437522478, -0.09389334619045257, 0.1366825156741672, 0.1388888888888889] \n",
      " action =  [-0.85779995 -0.42356807  5.        ] \n",
      " new state =  [0.004964763765499758, -0.2744291339355313, -0.41021513925593733, -0.4811552399559759, 0.2744291339355313, 0.695929424970223, -0.09627612382173538, 0.13550593770212596, 0.1527777777777778] \n",
      " distance =  0.4935711285180901 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -11 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.47040465 -0.19188574  4.99999619] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.004964763765499758, -0.2744291339355313, -0.41021513925593733, -0.4811552399559759, 0.2744291339355313, 0.695929424970223, -0.09627612382173538, 0.13550593770212596, 0.1527777777777778] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4803583337242614, 0.2413377056899913, 0.6827254587983915]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.004964763765499758, -0.2744291339355313, -0.41021513925593733, -0.4811552399559759, 0.2744291339355313, 0.695929424970223, -0.09627612382173538, 0.13550593770212596, 0.1527777777777778] \n",
      " action =  [-0.47040465 -0.19188574  4.99999619] \n",
      " new state =  [0.004167857533785235, -0.2413377056899913, -0.39701117308410583, -0.4803583337242614, 0.2413377056899913, 0.6827254587983915, -0.09758280341823895, 0.13497292175889014, 0.16666665607028538] \n",
      " distance =  0.46462794876949515 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -12 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.96838856  0.05927392  4.9999876 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.004167857533785235, -0.2413377056899913, -0.39701117308410583, -0.4803583337242614, 0.2413377056899913, 0.6827254587983915, -0.09758280341823895, 0.13497292175889014, 0.16666665607028538] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.48402741232276164, 0.20560439692224816, 0.6638111199393225]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.004167857533785235, -0.2413377056899913, -0.39701117308410583, -0.4803583337242614, 0.2413377056899913, 0.6827254587983915, -0.09758280341823895, 0.13497292175889014, 0.16666665607028538] \n",
      " action =  [-0.96838856  0.05927392  4.9999876 ] \n",
      " new state =  [0.007836936132285477, -0.20560439692224816, -0.3780968342250368, -0.48402741232276164, 0.20560439692224816, 0.6638111199393225, -0.10027277163333363, 0.1351375715393159, 0.18055551052093505] \n",
      " distance =  0.4304553422280851 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -13 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-1.73436224  0.57511109  4.99995899] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.007836936132285477, -0.20560439692224816, -0.3780968342250368, -0.48402741232276164, 0.20560439692224816, 0.6638111199393225, -0.10027277163333363, 0.1351375715393159, 0.18055551052093505] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.49482423304355877, 0.16395398278767725, 0.6371755499125563]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.007836936132285477, -0.20560439692224816, -0.3780968342250368, -0.48402741232276164, 0.20560439692224816, 0.6638111199393225, -0.10027277163333363, 0.1351375715393159, 0.18055551052093505] \n",
      " action =  [-1.73436224  0.57511109  4.99995899] \n",
      " new state =  [0.018633756853082606, -0.16395398278767725, -0.3514612641982706, -0.49482423304355877, 0.16395398278767725, 0.6371755499125563, -0.10509044453501701, 0.1367351023480296, 0.19444428549872506] \n",
      " distance =  0.38826942398062764 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -14 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-1.9798646   1.61667609  4.999825  ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.018633756853082606, -0.16395398278767725, -0.3514612641982706, -0.49482423304355877, 0.16395398278767725, 0.6371755499125563, -0.10509044453501701, 0.1367351023480296, 0.19444428549872506] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.506141390429486, 0.10989490169292436, 0.6072219647661905]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.018633756853082606, -0.16395398278767725, -0.3514612641982706, -0.49482423304355877, 0.16395398278767725, 0.6371755499125563, -0.10509044453501701, 0.1367351023480296, 0.19444428549872506] \n",
      " action =  [-1.9798646   1.61667609  4.999825  ] \n",
      " new state =  [0.029950914239009863, -0.10989490169292436, -0.3215076790519048, -0.506141390429486, 0.10989490169292436, 0.6072219647661905, -0.110590068416463, 0.14122586927066247, 0.20833268827862209] \n",
      " distance =  0.34108816216807153 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -15 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-1.25347209  1.56905353  4.99912977] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.029950914239009863, -0.10989490169292436, -0.3215076790519048, -0.506141390429486, 0.10989490169292436, 0.6072219647661905, -0.110590068416463, 0.14122586927066247, 0.20833268827862209] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.5053303688157349, 0.0595345556154918, 0.5799675736304793]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.029950914239009863, -0.10989490169292436, -0.3215076790519048, -0.506141390429486, 0.10989490169292436, 0.6072219647661905, -0.110590068416463, 0.14122586927066247, 0.20833268827862209] \n",
      " action =  [-1.25347209  1.56905353  4.99912977] \n",
      " new state =  [0.029139892625258734, -0.0595345556154918, -0.29425328791619365, -0.5053303688157349, 0.0595345556154918, 0.5799675736304793, -0.11407193533248372, 0.1455843513003654, 0.22221915986802843] \n",
      " distance =  0.30162641479823354 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -16 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.2509689   1.04436493  4.99315119] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.029139892625258734, -0.0595345556154918, -0.29425328791619365, -0.5053303688157349, 0.0595345556154918, 0.5799675736304793, -0.11407193533248372, 0.1455843513003654, 0.22221915986802843] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.49078065213710415, 0.019544446129248542, 0.5583123565653864]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.029139892625258734, -0.0595345556154918, -0.29425328791619365, -0.5053303688157349, 0.0595345556154918, 0.5799675736304793, -0.11407193533248372, 0.1455843513003654, 0.22221915986802843] \n",
      " action =  [-0.2509689   1.04436493  4.99315119] \n",
      " new state =  [0.014590175946627981, -0.019544446129248542, -0.27259807085110066, -0.49078065213710415, 0.019544446129248542, 0.5583123565653864, -0.11476907117499245, 0.14848536499258544, 0.23608902427885267] \n",
      " distance =  0.2736869869767183 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -17 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [0.67058635 0.8156513  4.94640112] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [0.014590175946627981, -0.019544446129248542, -0.27259807085110066, -0.49078065213710415, 0.019544446129248542, 0.5583123565653864, -0.11476907117499245, 0.14848536499258544, 0.23608902427885267] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4654278499046028, -0.013945470512218637, 0.5421371472772426]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [0.014590175946627981, -0.019544446129248542, -0.27259807085110066, -0.49078065213710415, 0.019544446129248542, 0.5583123565653864, -0.11476907117499245, 0.14848536499258544, 0.23608902427885267] \n",
      " action =  [0.67058635 0.8156513  4.94640112] \n",
      " new state =  [-0.010762626285873378, 0.013945470512218637, -0.2564228615629569, -0.4654278499046028, -0.013945470512218637, 0.5421371472772426, -0.1129063313206037, 0.150751063041389, 0.24982902738783094] \n",
      " distance =  0.2570272246368308 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -18 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [0.56529057 0.8982774  4.70999146] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.010762626285873378, 0.013945470512218637, -0.2564228615629569, -0.4654278499046028, -0.013945470512218637, 0.5421371472772426, -0.1129063313206037, 0.150751063041389, 0.24982902738783094] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4407411513575928, -0.044089165102538096, 0.5237495620558255]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.010762626285873378, 0.013945470512218637, -0.2564228615629569, -0.4654278499046028, -0.013945470512218637, 0.5421371472772426, -0.1129063313206037, 0.150751063041389, 0.24982902738783094] \n",
      " action =  [0.56529057 0.8982774  4.70999146] \n",
      " new state =  [-0.03544932483288338, 0.044089165102538096, -0.23803527634153976, -0.4407411513575928, -0.044089165102538096, 0.5237495620558255, -0.11133607973655064, 0.1532462780467338, 0.26291233698527017] \n",
      " distance =  0.2446656941492602 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -19 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [0.37381706 1.25173306 4.25196505] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.03544932483288338, 0.044089165102538096, -0.23803527634153976, -0.4407411513575928, -0.044089165102538096, 0.5237495620558255, -0.11133607973655064, 0.1532462780467338, 0.26291233698527017] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.4184636943005159, -0.07337131686156148, 0.5039136040440342]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.03544932483288338, 0.044089165102538096, -0.23803527634153976, -0.4407411513575928, -0.044089165102538096, 0.5237495620558255, -0.11133607973655064, 0.1532462780467338, 0.26291233698527017] \n",
      " action =  [0.37381706 1.25173306 4.25196505] \n",
      " new state =  [-0.05772678188996028, 0.07337131686156148, -0.21819931832974848, -0.4184636943005159, -0.07337131686156148, 0.5039136040440342, -0.11029769902427991, 0.1567233143374324, 0.2747233510017395] \n",
      " distance =  0.23733241246179926 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -20 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.52695793  0.77054358  3.78447437] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.05772678188996028, 0.07337131686156148, -0.21819931832974848, -0.4184636943005159, -0.07337131686156148, 0.5039136040440342, -0.11029769902427991, 0.1567233143374324, 0.2747233510017395] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.40545699237156335, -0.09278324219616316, 0.47921424733062806]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.05772678188996028, 0.07337131686156148, -0.21819931832974848, -0.4184636943005159, -0.07337131686156148, 0.5039136040440342, -0.11029769902427991, 0.1567233143374324, 0.2747233510017395] \n",
      " action =  [-0.52695793  0.77054358  3.78447437] \n",
      " new state =  [-0.07073348381891281, 0.09278324219616316, -0.19349996161634236, -0.40545699237156335, -0.09278324219616316, 0.47921424733062806, -0.11176147104965316, 0.15886371315767367, 0.28523577981524995] \n",
      " distance =  0.22595174465163628 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -21 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.47228006  0.47689581  2.84801149] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07073348381891281, 0.09278324219616316, -0.19349996161634236, -0.40545699237156335, -0.09278324219616316, 0.47921424733062806, -0.11176147104965316, 0.15886371315767367, 0.28523577981524995] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3956801004740286, -0.10466602652115425, 0.45991647490498766]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07073348381891281, 0.09278324219616316, -0.19349996161634236, -0.40545699237156335, -0.09278324219616316, 0.47921424733062806, -0.11176147104965316, 0.15886371315767367, 0.28523577981524995] \n",
      " action =  [-0.47228006  0.47689581  2.84801149] \n",
      " new state =  [-0.08051037571644759, 0.10466602652115425, -0.17420218919070196, -0.3956801004740286, -0.10466602652115425, 0.45991647490498766, -0.11307336009211011, 0.16018842373871142, 0.29314692285325794] \n",
      " distance =  0.2185939167144494 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -22 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.6576497   0.09530033  2.27273321] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08051037571644759, 0.10466602652115425, -0.17420218919070196, -0.3956801004740286, -0.10466602652115425, 0.45991647490498766, -0.11307336009211011, 0.16018842373871142, 0.29314692285325794] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3899958524985224, -0.11023709113757515, 0.4429232452754816]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08051037571644759, 0.10466602652115425, -0.17420218919070196, -0.3956801004740286, -0.10466602652115425, 0.45991647490498766, -0.11307336009211011, 0.16018842373871142, 0.29314692285325794] \n",
      " action =  [-0.6576497   0.09530033  2.27273321] \n",
      " new state =  [-0.08619462369195374, 0.11023709113757515, -0.1572089595611959, -0.3899958524985224, -0.11023709113757515, 0.4429232452754816, -0.11490016480286916, 0.1604531468823552, 0.29946007066302827] \n",
      " distance =  0.21046706721524222 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -23 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.62823701 -0.0822081   1.68494844] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08619462369195374, 0.11023709113757515, -0.1572089595611959, -0.3899958524985224, -0.11023709113757515, 0.4429232452754816, -0.11490016480286916, 0.1604531468823552, 0.29946007066302827] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38674142392887195, -0.11236888031484932, 0.42963602053443734]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08619462369195374, 0.11023709113757515, -0.1572089595611959, -0.3899958524985224, -0.11023709113757515, 0.4429232452754816, -0.11490016480286916, 0.1604531468823552, 0.29946007066302827] \n",
      " action =  [-0.62823701 -0.0822081   1.68494844] \n",
      " new state =  [-0.08944905226160421, 0.11236888031484932, -0.14392173482015164, -0.38674142392887195, -0.11236888031484932, 0.42963602053443734, -0.11664526760578156, 0.1602247910367118, 0.30414048300849067] \n",
      " distance =  0.2033257582485657 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -24 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.26592436 -0.1585457   1.21687603] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08944905226160421, 0.11236888031484932, -0.14392173482015164, -0.38674142392887195, -0.11236888031484932, 0.42963602053443734, -0.11664526760578156, 0.1602247910367118, 0.30414048300849067] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38296033218702047, -0.11272008074409204, 0.42148492272020155]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08944905226160421, 0.11236888031484932, -0.14392173482015164, -0.38674142392887195, -0.11236888031484932, 0.42963602053443734, -0.11664526760578156, 0.1602247910367118, 0.30414048300849067] \n",
      " action =  [-0.26592436 -0.1585457   1.21687603] \n",
      " new state =  [-0.0932301440034557, 0.11272008074409204, -0.13577063700591585, -0.38296033218702047, -0.11272008074409204, 0.42148492272020155, -0.11738394639558263, 0.15978438630700112, 0.30752069420284694] \n",
      " distance =  0.19957791016756307 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -25 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.71381992 -0.26387307  0.85093224] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0932301440034557, 0.11272008074409204, -0.13577063700591585, -0.38296033218702047, -0.11272008074409204, 0.42148492272020155, -0.11738394639558263, 0.15978438630700112, 0.30752069420284694] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3842814559933276, -0.11137695220867272, 0.4124947898217494]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0932301440034557, 0.11272008074409204, -0.13577063700591585, -0.38296033218702047, -0.11272008074409204, 0.42148492272020155, -0.11738394639558263, 0.15978438630700112, 0.30752069420284694] \n",
      " action =  [-0.71381992 -0.26387307  0.85093224] \n",
      " new state =  [-0.09190902019714858, 0.11137695220867272, -0.12678050410746372, -0.3842814559933276, -0.11137695220867272, 0.4124947898217494, -0.11936677950951788, 0.15905140555567213, 0.309884394870864] \n",
      " distance =  0.19215980250467438 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -26 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.84530205 -0.23722401  0.44676208] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.09190902019714858, 0.11137695220867272, -0.12678050410746372, -0.3842814559933276, -0.11137695220867272, 0.4124947898217494, -0.11936677950951788, 0.15905140555567213, 0.309884394870864] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3762613748262065, -0.10969076149193652, 0.4160262527064482]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.09190902019714858, 0.11137695220867272, -0.12678050410746372, -0.3842814559933276, -0.11137695220867272, 0.4124947898217494, -0.11936677950951788, 0.15905140555567213, 0.309884394870864] \n",
      " action =  [ 0.84530205 -0.23722401  0.44676208] \n",
      " new state =  [-0.09992910136426969, 0.10969076149193652, -0.13031196699216252, -0.3762613748262065, -0.10969076149193652, 0.4160262527064482, -0.11701871827244759, 0.15839244996507962, 0.31112540066242217] \n",
      " distance =  0.19748189080905076 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -27 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-2.35366964 -0.45654422  0.3841798 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.09992910136426969, 0.10969076149193652, -0.13031196699216252, -0.3762613748262065, -0.10969076149193652, 0.4160262527064482, -0.11701871827244759, 0.15839244996507962, 0.31112540066242217] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3916542640171566, -0.10577770033114972, 0.39882297236258213]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.09992910136426969, 0.10969076149193652, -0.13031196699216252, -0.3762613748262065, -0.10969076149193652, 0.4160262527064482, -0.11701871827244759, 0.15839244996507962, 0.31112540066242217] \n",
      " action =  [-2.35366964 -0.45654422  0.3841798 ] \n",
      " new state =  [-0.08453621217331958, 0.10577770033114972, -0.11310868664829643, -0.3916542640171566, -0.10577770033114972, 0.39882297236258213, -0.12355668950412009, 0.15712427157494757, 0.31219256677561336] \n",
      " distance =  0.17643374975117865 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -28 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 1.97362304 -0.3546648   0.1021523 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08453621217331958, 0.10577770033114972, -0.11310868664829643, -0.3916542640171566, -0.10577770033114972, 0.39882297236258213, -0.12355668950412009, 0.15712427157494757, 0.31219256677561336] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3776156906067246, -0.1024659731324901, 0.41199724936055626]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08453621217331958, 0.10577770033114972, -0.11310868664829643, -0.3916542640171566, -0.10577770033114972, 0.39882297236258213, -0.12355668950412009, 0.15712427157494757, 0.31219256677561336] \n",
      " action =  [ 1.97362304 -0.3546648   0.1021523 ] \n",
      " new state =  [-0.09857478558375155, 0.1024659731324901, -0.12628296364627056, -0.3776156906067246, -0.1024659731324901, 0.41199724936055626, -0.1180744032892916, 0.1561390915678607, 0.31247632317245005] \n",
      " distance =  0.19016742862581928 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -29 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-1.08459592 -0.28293908 -0.17594999] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.09857478558375155, 0.1024659731324901, -0.12628296364627056, -0.3776156906067246, -0.1024659731324901, 0.41199724936055626, -0.1180744032892916, 0.1561390915678607, 0.31247632317245005] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3865794854352333, -0.09943871458912079, 0.4060707484423263]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.09857478558375155, 0.1024659731324901, -0.12628296364627056, -0.3776156906067246, -0.1024659731324901, 0.41199724936055626, -0.1180744032892916, 0.1561390915678607, 0.31247632317245005] \n",
      " action =  [-1.08459592 -0.28293908 -0.17594999] \n",
      " new state =  [-0.08961099075524287, 0.09943871458912079, -0.12035646272804063, -0.3865794854352333, -0.09943871458912079, 0.4060707484423263, -0.12108716973000103, 0.15535314968890615, 0.3119875731981463] \n",
      " distance =  0.1800107378566042 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -30 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 1.01314688 -0.30941573 -0.16367601] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08961099075524287, 0.09943871458912079, -0.12035646272804063, -0.3865794854352333, -0.09943871458912079, 0.4060707484423263, -0.12108716973000103, 0.15535314968890615, 0.3119875731981463] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38050657765211093, -0.09615189629660714, 0.41411342213001956]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08961099075524287, 0.09943871458912079, -0.12035646272804063, -0.3865794854352333, -0.09943871458912079, 0.4060707484423263, -0.12108716973000103, 0.15535314968890615, 0.3119875731981463] \n",
      " action =  [ 1.01314688 -0.30941573 -0.16367601] \n",
      " new state =  [-0.09568389853836523, 0.09615189629660714, -0.12839913641573386, -0.38050657765211093, -0.09615189629660714, 0.41411342213001956, -0.1182728728486432, 0.154493661555979, 0.3115329176187515] \n",
      " distance =  0.18678044285534795 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -31 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.14876312 -0.25798547 -0.32584235] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.09568389853836523, 0.09615189629660714, -0.12839913641573386, -0.38050657765211093, -0.09615189629660714, 0.41411342213001956, -0.1182728728486432, 0.154493661555979, 0.3115329176187515] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3834839517266249, -0.09308487854845726, 0.4151848788227205]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.09568389853836523, 0.09615189629660714, -0.12839913641573386, -0.38050657765211093, -0.09615189629660714, 0.41411342213001956, -0.1182728728486432, 0.154493661555979, 0.3115329176187515] \n",
      " action =  [-0.14876312 -0.25798547 -0.32584235] \n",
      " new state =  [-0.09270652446385125, 0.09308487854845726, -0.12947059310843478, -0.3834839517266249, -0.09308487854845726, 0.4151848788227205, -0.11868610373801655, 0.1537770352429814, 0.3106277999778589] \n",
      " distance =  0.18445034229406454 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -32 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.0654318  -0.22959581 -0.29316407] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.09270652446385125, 0.09308487854845726, -0.12947059310843478, -0.3834839517266249, -0.09308487854845726, 0.4151848788227205, -0.11868610373801655, 0.1537770352429814, 0.3106277999778589] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38470608285671887, -0.09031892582297411, 0.4174635268630744]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.09270652446385125, 0.09308487854845726, -0.12947059310843478, -0.3834839517266249, -0.09308487854845726, 0.4151848788227205, -0.11868610373801655, 0.1537770352429814, 0.3106277999778589] \n",
      " action =  [ 0.0654318  -0.22959581 -0.29316407] \n",
      " new state =  [-0.0914843933337573, 0.09031892582297411, -0.13174924114878872, -0.38470608285671887, -0.09031892582297411, 0.4174635268630744, -0.11850434872839186, 0.15313926910360653, 0.30981345532668964] \n",
      " distance =  0.18407814951466453 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -33 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.03345413 -0.19637269 -0.28148547] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0914843933337573, 0.09031892582297411, -0.13174924114878872, -0.38470608285671887, -0.09031892582297411, 0.4174635268630744, -0.11850434872839186, 0.15313926910360653, 0.30981345532668964] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3865446799360348, -0.08787497869874657, 0.41896752285136757]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0914843933337573, 0.09031892582297411, -0.13174924114878872, -0.38470608285671887, -0.09031892582297411, 0.4174635268630744, -0.11850434872839186, 0.15313926910360653, 0.30981345532668964] \n",
      " action =  [-0.03345413 -0.19637269 -0.28148547] \n",
      " new state =  [-0.08964579625444136, 0.08787497869874657, -0.13325323713708187, -0.3865446799360348, -0.08787497869874657, 0.41896752285136757, -0.11859727687098914, 0.15259378941522705, 0.30903155124849746] \n",
      " distance =  0.18307049427723013 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -34 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.04000328 -0.16207758 -0.24280718] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08964579625444136, 0.08787497869874657, -0.13325323713708187, -0.3865446799360348, -0.08787497869874657, 0.41896752285136757, -0.11859727687098914, 0.15259378941522705, 0.30903155124849746] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38819960481034277, -0.08581969378879682, 0.42017225750361814]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08964579625444136, 0.08787497869874657, -0.13325323713708187, -0.3865446799360348, -0.08787497869874657, 0.41896752285136757, -0.11859727687098914, 0.15259378941522705, 0.30903155124849746] \n",
      " action =  [-0.04000328 -0.16207758 -0.24280718] \n",
      " new state =  [-0.0879908713801334, 0.08581969378879682, -0.13445797178933244, -0.38819960481034277, -0.08581969378879682, 0.42017225750361814, -0.11870839710657795, 0.15214357392655478, 0.3083570868604713] \n",
      " distance =  0.1821712366591909 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -35 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.03189249 -0.13331231 -0.20723701] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0879908713801334, 0.08581969378879682, -0.13445797178933244, -0.38819960481034277, -0.08581969378879682, 0.42017225750361814, -0.11870839710657795, 0.15214357392655478, 0.3083570868604713] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.38958719587413965, -0.0841014600658257, 0.4212035129389554]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0879908713801334, 0.08581969378879682, -0.13445797178933244, -0.38819960481034277, -0.08581969378879682, 0.42017225750361814, -0.11870839710657795, 0.15214357392655478, 0.3083570868604713] \n",
      " action =  [-0.03189249 -0.13331231 -0.20723701] \n",
      " new state =  [-0.08660328031633652, 0.0841014600658257, -0.13548922722466972, -0.38958719587413965, -0.0841014600658257, 0.4212035129389554, -0.11879698734523522, 0.15177326194114155, 0.3077814285125997] \n",
      " distance =  0.1814676677556964 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -36 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.02592376 -0.10894424 -0.17644408] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08660328031633652, 0.0841014600658257, -0.13548922722466972, -0.38958719587413965, -0.0841014600658257, 0.4212035129389554, -0.11879698734523522, 0.15177326194114155, 0.3077814285125997] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39075269192629747, -0.08267416772755032, 0.4220803244571956]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08660328031633652, 0.0841014600658257, -0.13548922722466972, -0.38958719587413965, -0.0841014600658257, 0.4212035129389554, -0.11879698734523522, 0.15177326194114155, 0.3077814285125997] \n",
      " action =  [-0.02592376 -0.10894424 -0.17644408] \n",
      " new state =  [-0.0854377842641787, 0.08267416772755032, -0.13636603874290992, -0.39075269192629747, -0.08267416772755032, 0.4220803244571956, -0.11886899780171613, 0.151470639059941, 0.30729130605856575] \n",
      " distance =  0.18091525505564277 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -37 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.02083204 -0.08839335 -0.14954402] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0854377842641787, 0.08267416772755032, -0.13636603874290992, -0.39075269192629747, -0.08267416772755032, 0.4220803244571956, -0.11886899780171613, 0.151470639059941, 0.30729130605856575] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39172662196134134, -0.08149701933967923, 0.42282385124810035]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0854377842641787, 0.08267416772755032, -0.13636603874290992, -0.39075269192629747, -0.08267416772755032, 0.4220803244571956, -0.11886899780171613, 0.151470639059941, 0.30729130605856575] \n",
      " action =  [-0.02083204 -0.08839335 -0.14954402] \n",
      " new state =  [-0.08446385422913483, 0.08149701933967923, -0.13710956553381465, -0.39172662196134134, -0.08149701933967923, 0.42282385124810035, -0.1189268645675232, 0.15122510198917655, 0.30687590601543585] \n",
      " distance =  0.18048528968689942 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -38 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.01657796 -0.07114138 -0.12605157] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08446385422913483, 0.08149701933967923, -0.13710956553381465, -0.39172662196134134, -0.08149701933967923, 0.42282385124810035, -0.1189268645675232, 0.15122510198917655, 0.30687590601543585] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3925359451115577, -0.08053378536905662, 0.42345159829700435]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08446385422913483, 0.08149701933967923, -0.13710956553381465, -0.39172662196134134, -0.08149701933967923, 0.42282385124810035, -0.1189268645675232, 0.15122510198917655, 0.30687590601543585] \n",
      " action =  [-0.01657796 -0.07114138 -0.12605157] \n",
      " new state =  [-0.08365453107891846, 0.08053378536905662, -0.13773731258271865, -0.3925359451115577, -0.08053378536905662, 0.42345159829700435, -0.11897291446415087, 0.15102748705281152, 0.3065257627516985] \n",
      " distance =  0.18015365228996208 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -39 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.01303658 -0.05674137 -0.10557529] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08365453107891846, 0.08053378536905662, -0.13773731258271865, -0.3925359451115577, -0.08053378536905662, 0.42345159829700435, -0.11897291446415087, 0.15102748705281152, 0.3065257627516985] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39320406030663463, -0.07975234693044923, 0.42397880099283125]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08365453107891846, 0.08053378536905662, -0.13773731258271865, -0.3925359451115577, -0.08053378536905662, 0.42345159829700435, -0.11897291446415087, 0.15102748705281152, 0.3065257627516985] \n",
      " action =  [-0.01303658 -0.05674137 -0.10557529] \n",
      " new state =  [-0.08298641588384154, 0.07975234693044923, -0.13826451527854555, -0.39320406030663463, -0.07975234693044923, 0.42397880099283125, -0.11900912719882197, 0.15086987214162945, 0.30623249804808034] \n",
      " distance =  0.17990068995802044 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -40 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.01013162 -0.04479962 -0.08780906] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08298641588384154, 0.07975234693044923, -0.13826451527854555, -0.39320406030663463, -0.07975234693044923, 0.42397880099283125, -0.11900912719882197, 0.15086987214162945, 0.30623249804808034] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3937517175524561, -0.07912431553282043, 0.4244187704024663]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08298641588384154, 0.07975234693044923, -0.13826451527854555, -0.39320406030663463, -0.07975234693044923, 0.42397880099283125, -0.11900912719882197, 0.15086987214162945, 0.30623249804808034] \n",
      " action =  [-0.01013162 -0.04479962 -0.08780906] \n",
      " new state =  [-0.08243875863802008, 0.07912431553282043, -0.13870448468818058, -0.3937517175524561, -0.07912431553282043, 0.4244187704024663, -0.11903727058735158, 0.15074542875712116, 0.3059885840035147] \n",
      " distance =  0.17971015638223897 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -41 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.00774908 -0.03496735 -0.07246985] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08243875863802008, 0.07912431553282043, -0.13870448468818058, -0.3937517175524561, -0.07912431553282043, 0.4244187704024663, -0.11903727058735158, 0.15074542875712116, 0.3059885840035147] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3941969562269774, -0.07862477423131724, 0.42478344703026927]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08243875863802008, 0.07912431553282043, -0.13870448468818058, -0.3937517175524561, -0.07912431553282043, 0.4244187704024663, -0.11903727058735158, 0.15074542875712116, 0.3059885840035147] \n",
      " action =  [-0.00774908 -0.03496735 -0.07246985] \n",
      " new state =  [-0.08199351996349874, 0.07862477423131724, -0.13906916131598357, -0.3941969562269774, -0.07862477423131724, 0.42478344703026927, -0.11905879581140147, 0.15064829722460774, 0.3057872788773643] \n",
      " distance =  0.17956899528610537 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -42 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.00582885 -0.02693653 -0.0593117 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08199351996349874, 0.07862477423131724, -0.13906916131598357, -0.3941969562269774, -0.07862477423131724, 0.42478344703026927, -0.11905879581140147, 0.15064829722460774, 0.3057872788773643] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3945557693351793, -0.07823197117000466, 0.4250833616954687]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08199351996349874, 0.07862477423131724, -0.13906916131598357, -0.3941969562269774, -0.07862477423131724, 0.42478344703026927, -0.11905879581140147, 0.15064829722460774, 0.3057872788773643] \n",
      " action =  [-0.00582885 -0.02693653 -0.0593117 ] \n",
      " new state =  [-0.08163470685529689, 0.07823197117000466, -0.13936907598118298, -0.3945557693351793, -0.07823197117000466, 0.4250833616954687, -0.1190749870509737, 0.15057347353237371, 0.30562252416792846] \n",
      " distance =  0.1794664481632805 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -43 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.00429261 -0.02043262 -0.0481065 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08163470685529689, 0.07823197117000466, -0.13936907598118298, -0.3945557693351793, -0.07823197117000466, 0.4250833616954687, -0.1190749870509737, 0.15057347353237371, 0.30562252416792846] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39484212129240326, -0.07792707749956629, 0.4253279564511785]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08163470685529689, 0.07823197117000466, -0.13936907598118298, -0.3945557693351793, -0.07823197117000466, 0.4250833616954687, -0.1190749870509737, 0.15057347353237371, 0.30562252416792846] \n",
      " action =  [-0.00429261 -0.02043262 -0.0481065 ] \n",
      " new state =  [-0.0813483548980729, 0.07792707749956629, -0.1396136707368928, -0.39484212129240326, -0.07792707749956629, 0.4253279564511785, -0.11908691097568307, 0.1505167162563238, 0.3054888950039943] \n",
      " distance =  0.17939387199365464 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -44 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.00307802 -0.01521594 -0.03863805] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0813483548980729, 0.07792707749956629, -0.1396136707368928, -0.39484212129240326, -0.07792707749956629, 0.4253279564511785, -0.11908691097568307, 0.1505167162563238, 0.3054888950039943] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3950681810863032, -0.07769390854431184, 0.42552562173860453]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0813483548980729, 0.07792707749956629, -0.1396136707368928, -0.39484212129240326, -0.07792707749956629, 0.4253279564511785, -0.11908691097568307, 0.1505167162563238, 0.3054888950039943] \n",
      " action =  [-0.00307802 -0.01521594 -0.03863805] \n",
      " new state =  [-0.08112229510417296, 0.07769390854431184, -0.13981133602431883, -0.3950681810863032, -0.07769390854431184, 0.42552562173860453, -0.11909546103302596, 0.15047444976193625, 0.3053815670828852] \n",
      " distance =  0.17934430537032786 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -45 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-0.00118416 -0.01148982 -0.03070461] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08112229510417296, 0.07769390854431184, -0.13981133602431883, -0.3950681810863032, -0.07769390854431184, 0.42552562173860453, -0.11909546103302596, 0.15047444976193625, 0.3053815670828852] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39523784527669276, -0.07751447468013066, 0.42569072126807905]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08112229510417296, 0.07769390854431184, -0.13981133602431883, -0.3950681810863032, -0.07769390854431184, 0.42552562173860453, -0.11909546103302596, 0.15047444976193625, 0.3053815670828852] \n",
      " action =  [-0.00118416 -0.01148982 -0.03070461] \n",
      " new state =  [-0.08095263091378341, 0.07751447468013066, -0.13997643555379335, -0.39523784527669276, -0.07751447468013066, 0.42569072126807905, -0.11909875036087922, 0.1504425336063529, 0.3052962765045878] \n",
      " distance =  0.17931877968340384 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -46 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.0040422  -0.01064198 -0.02430326] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08095263091378341, 0.07751447468013066, -0.13997643555379335, -0.39523784527669276, -0.07751447468013066, 0.42569072126807905, -0.11909875036087922, 0.1504425336063529, 0.3052962765045878] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39533651558260247, -0.07735660557915307, 0.42585724392374924]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08095263091378341, 0.07751447468013066, -0.13997643555379335, -0.39523784527669276, -0.07751447468013066, 0.42569072126807905, -0.11909875036087922, 0.1504425336063529, 0.3052962765045878] \n",
      " action =  [ 0.0040422  -0.01064198 -0.02430326] \n",
      " new state =  [-0.0808539606078737, 0.07735660557915307, -0.14014295820946354, -0.39533651558260247, -0.07735660557915307, 0.42585724392374924, -0.1190875220280658, 0.15041297254566516, 0.30522876743537686] \n",
      " distance =  0.17933615393558458 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -47 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00544142 -0.01103425 -0.02026453] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0808539606078737, 0.07735660557915307, -0.14014295820946354, -0.39533651558260247, -0.07735660557915307, 0.42585724392374924, -0.1190875220280658, 0.15041297254566516, 0.30522876743537686] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39540532429890946, -0.07720294325127634, 0.42601249887573417]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0808539606078737, 0.07735660557915307, -0.14014295820946354, -0.39533651558260247, -0.07735660557915307, 0.42585724392374924, -0.1190875220280658, 0.15041297254566516, 0.30522876743537686] \n",
      " action =  [ 0.00544142 -0.01103425 -0.02026453] \n",
      " new state =  [-0.08078515189156671, 0.07720294325127634, -0.14029821316144847, -0.39540532429890946, -0.07720294325127634, 0.42601249887573417, -0.11907240697958817, 0.15038232184273914, 0.30517247706237766] \n",
      " distance =  0.1793603184349831 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -48 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00544063 -0.01191774 -0.01782257] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08078515189156671, 0.07720294325127634, -0.14029821316144847, -0.39540532429890946, -0.07720294325127634, 0.42601249887573417, -0.11907240697958817, 0.15038232184273914, 0.30517247706237766] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3954629437767785, -0.07704524979070988, 0.4261557293358746]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08078515189156671, 0.07720294325127634, -0.14029821316144847, -0.39540532429890946, -0.07720294325127634, 0.42601249887573417, -0.11907240697958817, 0.15038232184273914, 0.30517247706237766] \n",
      " action =  [ 0.00544063 -0.01191774 -0.01782257] \n",
      " new state =  [-0.08072753241369768, 0.07704524979070988, -0.1404414436215889, -0.3954629437767785, -0.07704524979070988, 0.4261557293358746, -0.11905729410549006, 0.15034921701428378, 0.3051229699307846] \n",
      " distance =  0.1793786611931124 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -49 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00504076 -0.01298585 -0.01647537] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08072753241369768, 0.07704524979070988, -0.1404414436215889, -0.3954629437767785, -0.07704524979070988, 0.4261557293358746, -0.11905729410549006, 0.15034921701428378, 0.3051229699307846] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3955178678805287, -0.07687940685148867, 0.42629012794883986]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08072753241369768, 0.07704524979070988, -0.1404414436215889, -0.3954629437767785, -0.07704524979070988, 0.4261557293358746, -0.11905729410549006, 0.15034921701428378, 0.3051229699307846] \n",
      " action =  [ 0.00504076 -0.01298585 -0.01647537] \n",
      " new state =  [-0.08067260830994749, 0.07687940685148867, -0.14057584223455416, -0.3955178678805287, -0.07687940685148867, 0.42629012794883986, -0.11904329198748909, 0.15031314521458827, 0.30507720501369073] \n",
      " distance =  0.17938807192598172 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -50 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.0046264  -0.01412421 -0.01587978] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08067260830994749, 0.07687940685148867, -0.14057584223455416, -0.3955178678805287, -0.07687940685148867, 0.42629012794883986, -0.11904329198748909, 0.15031314521458827, 0.30507720501369073] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3955739431469136, -0.07670315056165565, 0.42641961747854024]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08067260830994749, 0.07687940685148867, -0.14057584223455416, -0.3955178678805287, -0.07687940685148867, 0.42629012794883986, -0.11904329198748909, 0.15031314521458827, 0.30507720501369073] \n",
      " action =  [ 0.0046264  -0.01412421 -0.01587978] \n",
      " new state =  [-0.08061653304356259, 0.07670315056165565, -0.14070533176425454, -0.3955739431469136, -0.07670315056165565, 0.42641961747854024, -0.11903044087941655, 0.15027391129018117, 0.30503309451871446] \n",
      " distance =  0.17938893247058685 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -51 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00432387 -0.01529808 -0.01581883] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08061653304356259, 0.07670315056165565, -0.14070533176425454, -0.3955739431469136, -0.07670315056165565, 0.42641961747854024, -0.11903044087941655, 0.15027391129018117, 0.30503309451871446] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3956329793467945, -0.07651500887014183, 0.4265478136894548]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08061653304356259, 0.07670315056165565, -0.14070533176425454, -0.3955739431469136, -0.07670315056165565, 0.42641961747854024, -0.11903044087941655, 0.15027391129018117, 0.30503309451871446] \n",
      " action =  [ 0.00432387 -0.01529808 -0.01581883] \n",
      " new state =  [-0.08055749684368169, 0.07651500887014183, -0.14083352797516913, -0.3956329793467945, -0.07651500887014183, 0.4265478136894548, -0.11901843013345367, 0.15023141662434986, 0.3049891533174863] \n",
      " distance =  0.17938266215565654 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -52 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00415325 -0.01650569 -0.01612996] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08055749684368169, 0.07651500887014183, -0.14083352797516913, -0.3956329793467945, -0.07651500887014183, 0.4265478136894548, -0.11901843013345367, 0.15023141662434986, 0.3049891533174863] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39569583657707225, -0.07631385724334046, 0.4266776115144316]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08055749684368169, 0.07651500887014183, -0.14083352797516913, -0.3956329793467945, -0.07651500887014183, 0.4265478136894548, -0.11901843013345367, 0.15023141662434986, 0.3049891533174863] \n",
      " action =  [ 0.00415325 -0.01650569 -0.01612996] \n",
      " new state =  [-0.08049463961340392, 0.07631385724334046, -0.14096332580014592, -0.39569583657707225, -0.07631385724334046, 0.4266776115144316, -0.11900689332590748, 0.15018556747430314, 0.3049443478592568] \n",
      " distance =  0.17937070840716107 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -53 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00409875 -0.0177589  -0.01673554] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08049463961340392, 0.07631385724334046, -0.14096332580014592, -0.39569583657707225, -0.07631385724334046, 0.4266776115144316, -0.11900689332590748, 0.15018556747430314, 0.3049443478592568] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39576312027788113, -0.07609860597381944, 0.42681139818656794]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08049463961340392, 0.07631385724334046, -0.14096332580014592, -0.39569583657707225, -0.07631385724334046, 0.4266776115144316, -0.11900689332590748, 0.15018556747430314, 0.3049443478592568] \n",
      " action =  [ 0.00409875 -0.0177589  -0.01673554] \n",
      " new state =  [-0.08042735591259503, 0.07609860597381944, -0.14109711247228224, -0.39576312027788113, -0.07609860597381944, 0.42681139818656794, -0.11899550791001982, 0.15013623718534494, 0.3048978602461931] \n",
      " distance =  0.17935426551455563 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -54 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00414461 -0.01906687 -0.01755286] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08042735591259503, 0.07609860597381944, -0.14109711247228224, -0.39576312027788113, -0.07609860597381944, 0.42681139818656794, -0.11899550791001982, 0.15013623718534494, 0.3048978602461931] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39583515408602143, -0.07586823885604774, 0.42695102402893104]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08042735591259503, 0.07609860597381944, -0.14109711247228224, -0.39576312027788113, -0.07609860597381944, 0.42681139818656794, -0.11899550791001982, 0.15013623718534494, 0.3048978602461931] \n",
      " action =  [ 0.00414461 -0.01906687 -0.01755286] \n",
      " new state =  [-0.08035532210445473, 0.07586823885604774, -0.14123673831464534, -0.39583515408602143, -0.07586823885604774, 0.42695102402893104, -0.1189839951099, 0.1500832736518027, 0.3048491022963491] \n",
      " distance =  0.17933427923124065 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -55 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00426493 -0.02044454 -0.01855227] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08035532210445473, 0.07586823885604774, -0.14123673831464534, -0.39583515408602143, -0.07586823885604774, 0.42695102402893104, -0.1189839951099, 0.1500832736518027, 0.3048491022963491] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.395912319765402, -0.07562164443052052, 0.42709801720974855]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08035532210445473, 0.07586823885604774, -0.14123673831464534, -0.39583515408602143, -0.07586823885604774, 0.42695102402893104, -0.1189839951099, 0.1500832736518027, 0.3048491022963491] \n",
      " action =  [ 0.00426493 -0.02044454 -0.01855227] \n",
      " new state =  [-0.08027815642507419, 0.07562164443052052, -0.14138373149546285, -0.395912319765402, -0.07562164443052052, 0.42709801720974855, -0.11897214806974969, 0.15002648326206125, 0.30479756821360854] \n",
      " distance =  0.17931139126381762 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -56 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00444982 -0.0219014  -0.0197065 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08027815642507419, 0.07562164443052052, -0.14138373149546285, -0.395912319765402, -0.07562164443052052, 0.42709801720974855, -0.11897214806974969, 0.15002648326206125, 0.30479756821360854] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3959949447778235, -0.07535766627429855, 0.42725370106016164]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08027815642507419, 0.07562164443052052, -0.14138373149546285, -0.395912319765402, -0.07562164443052052, 0.42709801720974855, -0.11897214806974969, 0.15002648326206125, 0.30479756821360854] \n",
      " action =  [ 0.00444982 -0.0219014  -0.0197065 ] \n",
      " new state =  [-0.08019553141265268, 0.07535766627429855, -0.14153941534587594, -0.3959949447778235, -0.07535766627429855, 0.42725370106016164, -0.11895978745823312, 0.14996564604290244, 0.3047428279494246] \n",
      " distance =  0.17928610437320203 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -57 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00468239 -0.0234499  -0.02099364] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08019553141265268, 0.07535766627429855, -0.14153941534587594, -0.3959949447778235, -0.07535766627429855, 0.42725370106016164, -0.11895978745823312, 0.14996564604290244, 0.3047428279494246] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3960833828642651, -0.07507506152688717, 0.4274191766886603]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08019553141265268, 0.07535766627429855, -0.14153941534587594, -0.3959949447778235, -0.07535766627429855, 0.42725370106016164, -0.11895978745823312, 0.14996564604290244, 0.3047428279494246] \n",
      " action =  [ 0.00468239 -0.0234499  -0.02099364] \n",
      " new state =  [-0.08010709332621108, 0.07507506152688717, -0.1417048909743746, -0.3960833828642651, -0.07507506152688717, 0.4274191766886603, -0.11894678081929062, 0.14990050743799657, 0.3046845122964846] \n",
      " distance =  0.17925871635850676 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -58 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00495728 -0.02510012 -0.02241652] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08010709332621108, 0.07507506152688717, -0.1417048909743746, -0.3960833828642651, -0.07507506152688717, 0.4274191766886603, -0.11894678081929062, 0.14990050743799657, 0.3046845122964846] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39617804647700827, -0.07477247249756196, 0.4275955300099624]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08010709332621108, 0.07507506152688717, -0.1417048909743746, -0.3960833828642651, -0.07507506152688717, 0.4274191766886603, -0.11894678081929062, 0.14990050743799657, 0.3046845122964846] \n",
      " action =  [ 0.00495728 -0.02510012 -0.02241652] \n",
      " new state =  [-0.08001242971346789, 0.07477247249756196, -0.14188124429567672, -0.39617804647700827, -0.07477247249756196, 0.4275955300099624, -0.11893301060061073, 0.14983078487890047, 0.3046222441778001] \n",
      " distance =  0.17922945917159042 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -59 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00526771 -0.02686135 -0.02396785] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.08001242971346789, 0.07477247249756196, -0.14188124429567672, -0.39617804647700827, -0.07477247249756196, 0.4275955300099624, -0.11893301060061073, 0.14983078487890047, 0.3046222441778001] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39627936660135504, -0.07444845535877502, 0.42778376728881246]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.08001242971346789, 0.07477247249756196, -0.14188124429567672, -0.39617804647700827, -0.07477247249756196, 0.4275955300099624, -0.11893301060061073, 0.14983078487890047, 0.3046222441778001] \n",
      " action =  [ 0.00526771 -0.02686135 -0.02396785] \n",
      " new state =  [-0.07991110958912112, 0.07444845535877502, -0.14206948157452676, -0.39627936660135504, -0.07444845535877502, 0.42778376728881246, -0.11891837808070704, 0.14975617000988375, 0.30455566682438884] \n",
      " distance =  0.17919848084157086 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -60 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00561412 -0.02874227 -0.0256491 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07991110958912112, 0.07444845535877502, -0.14206948157452676, -0.39627936660135504, -0.07444845535877502, 0.42778376728881246, -0.11891837808070704, 0.14975617000988375, 0.30455566682438884] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3963877806628847, -0.07410146692718318, 0.4279849103435884]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07991110958912112, 0.07444845535877502, -0.14206948157452676, -0.39627936660135504, -0.07444845535877502, 0.42778376728881246, -0.11891837808070704, 0.14975617000988375, 0.30455566682438884] \n",
      " action =  [ 0.00561412 -0.02874227 -0.0256491 ] \n",
      " new state =  [-0.07980269552759145, 0.07410146692718318, -0.14227062462930268, -0.3963877806628847, -0.07410146692718318, 0.4279849103435884, -0.1189027832976232, 0.14967633037465727, 0.30448441931253506] \n",
      " distance =  0.17916592378753787 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -61 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00599231 -0.03075546 -0.02747236] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07980269552759145, 0.07410146692718318, -0.14227062462930268, -0.3963877806628847, -0.07410146692718318, 0.4279849103435884, -0.1189027832976232, 0.14967633037465727, 0.30448441931253506] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39650382148438496, -0.07372980123074215, 0.4282000230305267]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07980269552759145, 0.07410146692718318, -0.14227062462930268, -0.3963877806628847, -0.07410146692718318, 0.4279849103435884, -0.1189027832976232, 0.14967633037465727, 0.30448441931253506] \n",
      " action =  [ 0.00599231 -0.03075546 -0.02747236] \n",
      " new state =  [-0.07968665470609121, 0.07372980123074215, -0.14248573731624098, -0.39650382148438496, -0.07372980123074215, 0.4282000230305267, -0.1188861379865557, 0.14959089854576935, 0.30440810720643235] \n",
      " distance =  0.1791318840026124 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -62 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00640541 -0.0329084  -0.02943805] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07968665470609121, 0.07372980123074215, -0.14248573731624098, -0.39650382148438496, -0.07372980123074215, 0.4282000230305267, -0.1188861379865557, 0.14959089854576935, 0.30440810720643235] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3966280004058372, -0.07333166479013085, 0.4284301953038928]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07968665470609121, 0.07372980123074215, -0.14248573731624098, -0.39650382148438496, -0.07372980123074215, 0.4282000230305267, -0.1188861379865557, 0.14959089854576935, 0.30440810720643235] \n",
      " action =  [ 0.00640541 -0.0329084  -0.02943805] \n",
      " new state =  [-0.07956247578463899, 0.07333166479013085, -0.14271590958960712, -0.3966280004058372, -0.07333166479013085, 0.4284301953038928, -0.11886834518388949, 0.14949948631692678, 0.3043263348336849] \n",
      " distance =  0.17909648646431425 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -63 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00684946 -0.03521488 -0.03156139] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07956247578463899, 0.07333166479013085, -0.14271590958960712, -0.3966280004058372, -0.07333166479013085, 0.4284301953038928, -0.11886834518388949, 0.14949948631692678, 0.3043263348336849] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39676093655039296, -0.07290507946570404, 0.4286765769673548]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07956247578463899, 0.07333166479013085, -0.14271590958960712, -0.3966280004058372, -0.07333166479013085, 0.4284301953038928, -0.11886834518388949, 0.14949948631692678, 0.3043263348336849] \n",
      " action =  [ 0.00684946 -0.03521488 -0.03156139] \n",
      " new state =  [-0.0794295396400832, 0.07290507946570404, -0.14296229125306908, -0.39676093655039296, -0.07290507946570404, 0.4286765769673548, -0.11884931889621334, 0.14940166720965256, 0.3042386642967661] \n",
      " distance =  0.17905981989174322 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -64 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00733312 -0.03768265 -0.03384415] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0794295396400832, 0.07290507946570404, -0.14296229125306908, -0.39676093655039296, -0.07290507946570404, 0.4286765769673548, -0.11884931889621334, 0.14940166720965256, 0.3042386642967661] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.396903190837473, -0.0724479706284971, 0.42894038761078096]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0794295396400832, 0.07290507946570404, -0.14296229125306908, -0.39676093655039296, -0.07290507946570404, 0.4286765769673548, -0.11884931889621334, 0.14940166720965256, 0.3042386642967661] \n",
      " action =  [ 0.00733312 -0.03768265 -0.03384415] \n",
      " new state =  [-0.07928728535300317, 0.0724479706284971, -0.14322610189649526, -0.396903190837473, -0.0724479706284971, 0.42894038761078096, -0.11882894912786368, 0.14929699319570014, 0.30414465275696584] \n",
      " distance =  0.1790220610184716 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -65 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00785272 -0.04032669 -0.03630541] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07928728535300317, 0.0724479706284971, -0.14322610189649526, -0.396903190837473, -0.0724479706284971, 0.42894038761078096, -0.11882894912786368, 0.14929699319570014, 0.30414465275696584] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39705544689302197, -0.07195805422245585, 0.4292229281239747]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07928728535300317, 0.0724479706284971, -0.14322610189649526, -0.396903190837473, -0.0724479706284971, 0.42894038761078096, -0.11882894912786368, 0.14929699319570014, 0.30414465275696584] \n",
      " action =  [ 0.00785272 -0.04032669 -0.03630541] \n",
      " new state =  [-0.0791350292974542, 0.07195805422245585, -0.14350864240968902, -0.39705544689302197, -0.07195805422245585, 0.4292229281239747, -0.11880713602165795, 0.14918497460635585, 0.3040438044091894] \n",
      " distance =  0.17898336480148883 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -66 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00840741 -0.04316163 -0.03896049] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0791350292974542, 0.07195805422245585, -0.14350864240968902, -0.39705544689302197, -0.07195805422245585, 0.4292229281239747, -0.11880713602165795, 0.14918497460635585, 0.3040438044091894] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3972184699793211, -0.07143284587256214, 0.42952557867727637]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0791350292974542, 0.07195805422245585, -0.14350864240968902, -0.39705544689302197, -0.07195805422245585, 0.4292229281239747, -0.11880713602165795, 0.14918497460635585, 0.3040438044091894] \n",
      " action =  [ 0.00840741 -0.04316163 -0.03896049] \n",
      " new state =  [-0.07897200621115508, 0.07143284587256214, -0.14381129296299067, -0.3972184699793211, -0.07143284587256214, 0.42952557867727637, -0.11878378209657967, 0.14906508119828585, 0.30393558082481226] \n",
      " distance =  0.17894389405105462 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -67 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00900625 -0.04619917 -0.04182222] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07897200621115508, 0.07143284587256214, -0.14381129296299067, -0.3972184699793211, -0.07143284587256214, 0.42952557867727637, -0.11878378209657967, 0.14906508119828585, 0.30393558082481226] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.397393016048996, -0.07086969278978823, 0.4298498499129082]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07897200621115508, 0.07143284587256214, -0.14381129296299067, -0.3972184699793211, -0.07143284587256214, 0.42952557867727637, -0.11878378209657967, 0.14906508119828585, 0.30393558082481226] \n",
      " action =  [ 0.00900625 -0.04619917 -0.04182222] \n",
      " new state =  [-0.07879746014148015, 0.07086969278978823, -0.1441355641986225, -0.397393016048996, -0.07086969278978823, 0.4298498499129082, -0.11875876473366387, 0.14893675016291025, 0.3038194079779916] \n",
      " distance =  0.1789039237907384 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -68 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00965214 -0.0494541  -0.04490563] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07879746014148015, 0.07086969278978823, -0.1441355641986225, -0.397393016048996, -0.07086969278978823, 0.4298498499129082, -0.11875876473366387, 0.14893675016291025, 0.3038194079779916] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3975798887853519, -0.07026573448696279, 0.43019735314580193]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07879746014148015, 0.07086969278978823, -0.1441355641986225, -0.397393016048996, -0.07086969278978823, 0.4298498499129082, -0.11875876473366387, 0.14893675016291025, 0.3038194079779916] \n",
      " action =  [ 0.00965214 -0.0494541  -0.04490563] \n",
      " new state =  [-0.07861058740512428, 0.07026573448696279, -0.14448306743151623, -0.3975798887853519, -0.07026573448696279, 0.43019735314580193, -0.11873195323472222, 0.14879937765136775, 0.30369467010928525] \n",
      " distance =  0.17886378803321568 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -69 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01034307 -0.05294639 -0.04824007] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07861058740512428, 0.07026573448696279, -0.14448306743151623, -0.3975798887853519, -0.07026573448696279, 0.43019735314580193, -0.11873195323472222, 0.14879937765136775, 0.30369467010928525] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3977800477813522, -0.06961781408081617, 0.4305698440975658]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07861058740512428, 0.07026573448696279, -0.14448306743151623, -0.3975798887853519, -0.07026573448696279, 0.43019735314580193, -0.11873195323472222, 0.14879937765136775, 0.30369467010928525] \n",
      " action =  [ 0.01034307 -0.05294639 -0.04824007] \n",
      " new state =  [-0.07841042840912399, 0.06961781408081617, -0.14485555838328013, -0.3977800477813522, -0.06961781408081617, 0.4305698440975658, -0.11870322249208888, 0.1486523043493637, 0.3035606699064374] \n",
      " distance =  0.17882384660672448 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -70 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01108872 -0.05669071 -0.05183566] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07841042840912399, 0.06961781408081617, -0.14485555838328013, -0.3977800477813522, -0.06961781408081617, 0.4305698440975658, -0.11870322249208888, 0.1486523043493637, 0.3035606699064374] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3979944197657086, -0.06892256976700856, 0.4309691958033082]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07841042840912399, 0.06961781408081617, -0.14485555838328013, -0.3977800477813522, -0.06961781408081617, 0.4305698440975658, -0.11870322249208888, 0.1486523043493637, 0.3035606699064374] \n",
      " action =  [ 0.01108872 -0.05669071 -0.05183566] \n",
      " new state =  [-0.07819605642476757, 0.06892256976700856, -0.14525491008902253, -0.3979944197657086, -0.06892256976700856, 0.4309691958033082, -0.11867242049839762, 0.14849483016102263, 0.3034166819622947] \n",
      " distance =  0.17878459880158473 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -71 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00924456 -0.06107804 -0.052787  ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07819605642476757, 0.06892256976700856, -0.14525491008902253, -0.3979944197657086, -0.06892256976700856, 0.4309691958033082, -0.11867242049839762, 0.14849483016102263, 0.3034166819622947] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39823006897697305, -0.06817910566180513, 0.4313639889215831]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07819605642476757, 0.06892256976700856, -0.14525491008902253, -0.3979944197657086, -0.06892256976700856, 0.4309691958033082, -0.11867242049839762, 0.14849483016102263, 0.3034166819622947] \n",
      " action =  [ 0.00924456 -0.06107804 -0.052787  ] \n",
      " new state =  [-0.07796040721350311, 0.06817910566180513, -0.14564970320729742, -0.39823006897697305, -0.06817910566180513, 0.4313639889215831, -0.11864674117209183, 0.1483251689338229, 0.30327005141104263] \n",
      " distance =  0.178717798738997 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -72 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00778294 -0.06550992 -0.05408936] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07796040721350311, 0.06817910566180513, -0.14564970320729742, -0.39823006897697305, -0.06817910566180513, 0.4313639889215831, -0.11864674117209183, 0.1483251689338229, 0.30327005141104263] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39848575400299135, -0.06738603123597105, 0.4317586062131242]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07796040721350311, 0.06817910566180513, -0.14564970320729742, -0.39823006897697305, -0.06817910566180513, 0.4313639889215831, -0.11864674117209183, 0.1483251689338229, 0.30327005141104263] \n",
      " action =  [ 0.00778294 -0.06550992 -0.05408936] \n",
      " new state =  [-0.07770472218748481, 0.06738603123597105, -0.14604432049883848, -0.39848575400299135, -0.06738603123597105, 0.4317586062131242, -0.1186251218844619, 0.14814319694673436, 0.30311980318898957] \n",
      " distance =  0.1786276703255599 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -73 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00702481 -0.06998204 -0.05622816] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07770472218748481, 0.06738603123597105, -0.14604432049883848, -0.39848575400299135, -0.06738603123597105, 0.4317586062131242, -0.1186251218844619, 0.14814319694673436, 0.30311980318898957] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3987601298286534, -0.06654087030814516, 0.43216219229154856]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07770472218748481, 0.06738603123597105, -0.14604432049883848, -0.39848575400299135, -0.06738603123597105, 0.4317586062131242, -0.1186251218844619, 0.14814319694673436, 0.30311980318898957] \n",
      " action =  [ 0.00702481 -0.06998204 -0.05622816] \n",
      " new state =  [-0.07743034636182278, 0.06654087030814516, -0.14644790657726287, -0.3987601298286534, -0.06654087030814516, 0.43216219229154856, -0.11860560851766624, 0.14794880237895996, 0.3029636138532725] \n",
      " distance =  0.1785220863085012 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -74 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.00779803 -0.07343347 -0.05655371] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07743034636182278, 0.06654087030814516, -0.14644790657726287, -0.3987601298286534, -0.06654087030814516, 0.43216219229154856, -0.11860560851766624, 0.14794880237895996, 0.3029636138532725] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39903193836137807, -0.06565804238570239, 0.4325748831716283]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07743034636182278, 0.06654087030814516, -0.14644790657726287, -0.3987601298286534, -0.06654087030814516, 0.43216219229154856, -0.11860560851766624, 0.14794880237895996, 0.3029636138532725] \n",
      " action =  [ 0.00779803 -0.07343347 -0.05655371] \n",
      " new state =  [-0.0771585378290981, 0.06565804238570239, -0.1468605974573426, -0.39903193836137807, -0.06565804238570239, 0.4325748831716283, -0.11858394731922696, 0.14774482050755372, 0.3028065202136834] \n",
      " distance =  0.17841651710358733 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -75 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01292557 -0.07209781 -0.04692562] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0771585378290981, 0.06565804238570239, -0.1468605974573426, -0.39903193836137807, -0.06565804238570239, 0.4325748831716283, -0.11858394731922696, 0.14774482050755372, 0.3028065202136834] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3992164520166739, -0.06480957275342204, 0.432970854178141]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0771585378290981, 0.06565804238570239, -0.1468605974573426, -0.39903193836137807, -0.06565804238570239, 0.4325748831716283, -0.11858394731922696, 0.14774482050755372, 0.3028065202136834] \n",
      " action =  [ 0.01292557 -0.07209781 -0.04692562] \n",
      " new state =  [-0.07697402417380228, 0.06480957275342204, -0.14725656846385532, -0.3992164520166739, -0.06480957275342204, 0.432970854178141, -0.11854804295063433, 0.14754454881832418, 0.3026761712713374] \n",
      " distance =  0.17835295925142455 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -76 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01356378 -0.07249372 -0.0416461 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07697402417380228, 0.06480957275342204, -0.14725656846385532, -0.3992164520166739, -0.06480957275342204, 0.432970854178141, -0.11854804295063433, 0.14754454881832418, 0.3026761712713374] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.399369956068934, -0.0639680418320029, 0.4333431415160904]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07697402417380228, 0.06480957275342204, -0.14725656846385532, -0.3992164520166739, -0.06480957275342204, 0.432970854178141, -0.11854804295063433, 0.14754454881832418, 0.3026761712713374] \n",
      " action =  [ 0.01356378 -0.07249372 -0.0416461 ] \n",
      " new state =  [-0.07682052012154217, 0.0639680418320029, -0.14762885580180468, -0.399369956068934, -0.0639680418320029, 0.4333431415160904, -0.11851036577330282, 0.14734317736131036, 0.30256048765861326] \n",
      " distance =  0.17829072256546377 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -77 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01292758 -0.07345125 -0.03876038] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07682052012154217, 0.0639680418320029, -0.14762885580180468, -0.399369956068934, -0.0639680418320029, 0.4333431415160904, -0.11851036577330282, 0.14734317736131036, 0.30256048765861326] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3995140877354628, -0.06312231216225578, 0.4336958544206813]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07682052012154217, 0.0639680418320029, -0.14762885580180468, -0.399369956068934, -0.0639680418320029, 0.4333431415160904, -0.11851036577330282, 0.14734317736131036, 0.30256048765861326] \n",
      " action =  [ 0.01292758 -0.07345125 -0.03876038] \n",
      " new state =  [-0.07667638845501334, 0.06312231216225578, -0.14798156870639562, -0.3995140877354628, -0.06312231216225578, 0.4336958544206813, -0.11847445582194875, 0.1471391461091116, 0.30245281993928885] \n",
      " distance =  0.17821963841288382 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -78 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01212684 -0.07454851 -0.03725724] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07667638845501334, 0.06312231216225578, -0.14798156870639562, -0.3995140877354628, -0.06312231216225578, 0.4336958544206813, -0.11847445582194875, 0.1471391461091116, 0.30245281993928885] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3996568672530383, -0.06226780942074533, 0.43403516334733877]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07667638845501334, 0.06312231216225578, -0.14798156870639562, -0.3995140877354628, -0.06312231216225578, 0.4336958544206813, -0.11847445582194875, 0.1471391461091116, 0.30245281993928885] \n",
      " action =  [ 0.01212684 -0.07454851 -0.03725724] \n",
      " new state =  [-0.07653360893743788, 0.06226780942074533, -0.14832087763305307, -0.3996568672530383, -0.06226780942074533, 0.43403516334733877, -0.11844077016382168, 0.1469320669071749, 0.30234932760811517] \n",
      " distance =  0.17813970957898756 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -79 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01149265 -0.07564693 -0.03657927] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07653360893743788, 0.06226780942074533, -0.14832087763305307, -0.3996568672530383, -0.06226780942074533, 0.43403516334733877, -0.11844077016382168, 0.1469320669071749, 0.30234932760811517] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.39980099814306047, -0.06140262108493574, 0.4343665080528105]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.07653360893743788, 0.06226780942074533, -0.14832087763305307, -0.3996568672530383, -0.06226780942074533, 0.43403516334733877, -0.11844077016382168, 0.1469320669071749, 0.30234932760811517] \n",
      " action =  [ 0.01149265 -0.07564693 -0.03657927] \n",
      " new state =  [-0.0763894780474157, 0.06140262108493574, -0.14865222233852482, -0.39980099814306047, -0.06140262108493574, 0.4343665080528105, -0.1184088461437366, 0.14672193654761131, 0.30224771852501564] \n",
      " distance =  0.17805369257232218 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -80 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01106569 -0.07671614 -0.0364115 ] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.0763894780474157, 0.06140262108493574, -0.14865222233852482, -0.39980099814306047, -0.06140262108493574, 0.4343665080528105, -0.1184088461437366, 0.14672193654761131, 0.30224771852501564] \n",
      "\n",
      "\n",
      "defaultEndPosition = [0, 21, 0]; rotated endPosition = [-0.3999473221581062, -0.0605258490760506, 0.43469393774395054]; cubePosition = [-0.47619047619047616, 0.0, 0.2857142857142857]\n",
      "\n",
      " old state =  [-0.0763894780474157, 0.06140262108493574, -0.14865222233852482, -0.39980099814306047, -0.06140262108493574, 0.4343665080528105, -0.1184088461437366, 0.14672193654761131, 0.30224771852501564] \n",
      " action =  [ 0.01106569 -0.07671614 -0.0364115 ] \n",
      " new state =  [-0.07624315403236998, 0.0605258490760506, -0.14897965202966484, -0.3999473221581062, -0.0605258490760506, 0.43469393774395054, -0.11837810810344915, 0.1465088361589652, 0.3021465754757325] \n",
      " distance =  0.17796441684241424 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -81 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 0.01082076 -0.07775588 -0.03657331] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STATE =  [-0.07624315403236998, 0.0605258490760506, -0.14897965202966484, -0.3999473221581062, -0.0605258490760506, 0.43469393774395054, -0.11837810810344915, 0.1465088361589652, 0.3021465754757325] \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# применяем углы к сервам\u001b[39;00m\n\u001b[0;32m     20\u001b[0m servo1, servo2, servo3 \u001b[38;5;241m=\u001b[39m set_action_test(action, servo1, servo2, servo3)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# получаем обновленный state\u001b[39;00m\n\u001b[0;32m     25\u001b[0m next_state, next_goal \u001b[38;5;241m=\u001b[39m read_state_test(servo1, servo2, servo3)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes = 1\n",
    "\n",
    "servo1 = Servo()\n",
    "servo2 = Servo()\n",
    "servo3 = Servo()\n",
    "\n",
    "distances = []\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/without_her_rotate_scaled.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    state, goal = read_state_test(servo1, servo2, servo3)\n",
    "    \n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action([state], [goal], 0)\n",
    "        print(\"\\n\\nACTION = \", action, \"\\n\\n\")\n",
    "        print(\"\\n\\nSTATE = \", state, \"\\n\\n\")\n",
    "        \n",
    "        # применяем углы к сервам\n",
    "        servo1, servo2, servo3 = set_action_test(action, servo1, servo2, servo3)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # получаем обновленный state\n",
    "        next_state, next_goal = read_state_test(servo1, servo2, servo3)\n",
    "        \n",
    "        done, distance = check_if_done(next_state[3:6], goal)\n",
    "        \n",
    "        reward = -1\n",
    "        \n",
    "        if done:\n",
    "            reward += 10\n",
    "        \n",
    "        r += reward\n",
    "        \n",
    "        print(\"\\n old state = \", state, \"\\n action = \", action, \"\\n new state = \", next_state, \"\\n distance = \", distance,\n",
    "              \"\\n current reward = \", reward, \"\\n cumulative reward = \", r, \"\\n\")\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "         \n",
    "        distances.append(distance)\n",
    "        plt.plot(distances, label=\"ddpg+her\")\n",
    "        plt.legend()\n",
    "        plt.title(\"epoch success rate (over 20 episodes)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b9b17-5cc5-47c3-8de0-acd2966ced0a",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a644e7e5-9232-45e1-adf2-04fb959be521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Servo1.MoveServo(120)\n",
    "Servo2.MoveServo(120)\n",
    "Servo3.MoveServo(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ebc68-9429-4759-a0c6-b52caca0048c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate_scaled.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/without_her_rotate_scaled.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " firstAngle =  -0.480000000000004  secondAngle =  -0.23999999999999488  thirdAngle =  -0.23999999999999488 \n",
      "\n",
      "not normalize end pos: [-0.0003509073637783973, 1.0002807333103656, 0.04188550121170425]\n",
      "\n",
      "\n",
      "ACTION =  [-4.96324921  5.          4.75140619]\n",
      "\n",
      "\n",
      "STATE =  [-0.007567160228492194, -0.21047210152929707, 0.5881811298379541, -0.0003509073637783973, 1.0002807333103656, 0.04188550121170425, -0.0013333333333333443, -0.0006666666666666524, -0.0006666666666666524]\n",
      "\n",
      " firstAngle =  -5.760000000000005  secondAngle =  4.799999999999997  thirdAngle =  -4.560000000000002 \n",
      "\n",
      "not normalize end pos: [-0.0961522998965016, 0.9580108136515386, 0.9532203461144535]\n",
      "\n",
      " old state =  [-0.007567160228492194, -0.21047210152929707, 0.5881811298379541, -0.0003509073637783973, 1.0002807333103656, 0.04188550121170425, -0.0013333333333333443, -0.0006666666666666524, -0.0006666666666666524] \n",
      " action =  [-4.96324921  5.          4.75140619] \n",
      " new state =  [0.08823423230423101, -0.1682021818704701, -0.3231537150647952, -0.0961522998965016, 0.9580108136515386, 0.9532203461144535, -0.016000000000000014, 0.013333333333333326, -0.012666666666666673] \n",
      " distance =  0.3748407359085756 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -1 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 4.81275749 -0.34229296  4.99996662]\n",
      "\n",
      "\n",
      "STATE =  [0.08823423230423101, -0.1682021818704701, -0.3231537150647952, -0.0961522998965016, 0.9580108136515386, 0.9532203461144535, -0.016000000000000014, 0.013333333333333326, -0.012666666666666673]\n",
      "\n",
      " firstAngle =  -1.6800000000000068  secondAngle =  4.799999999999997  thirdAngle =  -10.560000000000002 \n",
      "\n",
      "not normalize end pos: [-0.06180424416245591, 1.01345312419975, 2.1072068208833215]\n",
      "\n",
      " old state =  [0.08823423230423101, -0.1682021818704701, -0.3231537150647952, -0.0961522998965016, 0.9580108136515386, 0.9532203461144535, -0.016000000000000014, 0.013333333333333326, -0.012666666666666673] \n",
      " action =  [ 4.81275749 -0.34229296  4.99996662] \n",
      " new state =  [0.053886176570185323, -0.22364449241868145, -1.4771401898336631, -0.06180424416245591, 1.01345312419975, 2.1072068208833215, -0.004666666666666685, 0.013333333333333326, -0.02933333333333334] \n",
      " distance =  1.494946059039052 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -2 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-1.50463915 -5.          5.        ]\n",
      "\n",
      "\n",
      "STATE =  [0.053886176570185323, -0.22364449241868145, -1.4771401898336631, -0.06180424416245591, 1.01345312419975, 2.1072068208833215, -0.004666666666666685, 0.013333333333333326, -0.02933333333333334]\n",
      "\n",
      " firstAngle =  -3.1200000000000045  secondAngle =  -0.480000000000004  thirdAngle =  -15.599999999999994 \n",
      "\n",
      "not normalize end pos: [-0.1603561925286305, 1.429944035675883, 2.941874965353036]\n",
      "\n",
      " old state =  [0.053886176570185323, -0.22364449241868145, -1.4771401898336631, -0.06180424416245591, 1.01345312419975, 2.1072068208833215, -0.004666666666666685, 0.013333333333333326, -0.02933333333333334] \n",
      " action =  [-1.50463915 -5.          5.        ] \n",
      " new state =  [0.15243812493635991, -0.6401354038948145, -2.311808334303378, -0.1603561925286305, 1.429944035675883, 2.941874965353036, -0.00866666666666668, -0.0013333333333333443, -0.04333333333333332] \n",
      " distance =  2.4036365140778355 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -3 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 4.98942327 -5.          5.        ]\n",
      "\n",
      "\n",
      "STATE =  [0.15243812493635991, -0.6401354038948145, -2.311808334303378, -0.1603561925286305, 1.429944035675883, 2.941874965353036, -0.00866666666666668, -0.0013333333333333443, -0.04333333333333332]\n",
      "\n",
      " firstAngle =  1.9200000000000017  secondAngle =  -6.0  thirdAngle =  -20.159999999999997 \n",
      "\n",
      "not normalize end pos: [0.12045815700236587, 2.0610325619465075, 3.593312704230603]\n",
      "\n",
      " old state =  [0.15243812493635991, -0.6401354038948145, -2.311808334303378, -0.1603561925286305, 1.429944035675883, 2.941874965353036, -0.00866666666666668, -0.0013333333333333443, -0.04333333333333332] \n",
      " action =  [ 4.98942327 -5.          5.        ] \n",
      " new state =  [-0.12837622459463646, -1.2712239301654389, -2.9632460731809447, 0.12045815700236587, 2.0610325619465075, 3.593312704230603, 0.005333333333333338, -0.016666666666666666, -0.05599999999999999] \n",
      " distance =  3.226967310942075 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -4 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-4.19615269 -5.          5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-0.12837622459463646, -1.2712239301654389, -2.9632460731809447, 0.12045815700236587, 2.0610325619465075, 3.593312704230603, 0.005333333333333338, -0.016666666666666666, -0.05599999999999999]\n",
      "\n",
      " firstAngle =  -2.4000000000000057  secondAngle =  -11.519999999999996  thirdAngle =  -25.92 \n",
      "\n",
      "not normalize end pos: [-0.17967643313179626, 3.024366243484773, 4.286949823723726]\n",
      "\n",
      " old state =  [-0.12837622459463646, -1.2712239301654389, -2.9632460731809447, 0.12045815700236587, 2.0610325619465075, 3.593312704230603, 0.005333333333333338, -0.016666666666666666, -0.05599999999999999] \n",
      " action =  [-4.19615269 -5.          5.        ] \n",
      " new state =  [0.17175836553952567, -2.2345576117037043, -3.6568831926740675, -0.17967643313179626, 3.024366243484773, 4.286949823723726, -0.006666666666666683, -0.03199999999999999, -0.07200000000000001] \n",
      " distance =  4.289002604454543 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -5 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 5. -5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [0.17175836553952567, -2.2345576117037043, -3.6568831926740675, -0.17967643313179626, 3.024366243484773, 4.286949823723726, -0.006666666666666683, -0.03199999999999999, -0.07200000000000001]\n",
      "\n",
      " firstAngle =  2.1599999999999966  secondAngle =  -17.040000000000006  thirdAngle =  -30.72 \n",
      "\n",
      "not normalize end pos: [0.1744007296997946, 4.078591255386993, 4.623931243211049]\n",
      "\n",
      " old state =  [0.17175836553952567, -2.2345576117037043, -3.6568831926740675, -0.17967643313179626, 3.024366243484773, 4.286949823723726, -0.006666666666666683, -0.03199999999999999, -0.07200000000000001] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [-0.18231879729206518, -3.288782623605924, -3.993864612161391, 0.1744007296997946, 4.078591255386993, 4.623931243211049, 0.005999999999999991, -0.04733333333333335, -0.08533333333333333] \n",
      " distance =  5.176899248532209 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -6 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 5. -5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [-0.18231879729206518, -3.288782623605924, -3.993864612161391, 0.1744007296997946, 4.078591255386993, 4.623931243211049, 0.005999999999999991, -0.04733333333333335, -0.08533333333333333]\n",
      "\n",
      " firstAngle =  6.719999999999999  secondAngle =  -23.040000000000006  thirdAngle =  -36.239999999999995 \n",
      "\n",
      "not normalize end pos: [0.5569946972584634, 5.423508817865985, 4.727228694833632]\n",
      "\n",
      " old state =  [-0.18231879729206518, -3.288782623605924, -3.993864612161391, 0.1744007296997946, 4.078591255386993, 4.623931243211049, 0.005999999999999991, -0.04733333333333335, -0.08533333333333333] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [-0.5649127648507339, -4.6337001860849165, -4.097162063783974, 0.5569946972584634, 5.423508817865985, 4.727228694833632, 0.018666666666666665, -0.06400000000000002, -0.10066666666666665] \n",
      " distance =  6.211041846850272 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -7 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 5. -5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [-0.5649127648507339, -4.6337001860849165, -4.097162063783974, 0.5569946972584634, 5.423508817865985, 4.727228694833632, 0.018666666666666665, -0.06400000000000002, -0.10066666666666665]\n",
      "\n",
      " firstAngle =  11.280000000000001  secondAngle =  -28.319999999999993  thirdAngle =  -41.040000000000006 \n",
      "\n",
      "not normalize end pos: [0.9000131667157665, 6.686296132500578, 4.512322662877144]\n",
      "\n",
      " old state =  [-0.5649127648507339, -4.6337001860849165, -4.097162063783974, 0.5569946972584634, 5.423508817865985, 4.727228694833632, 0.018666666666666665, -0.06400000000000002, -0.10066666666666665] \n",
      " action =  [ 5. -5.  5.] \n",
      " new state =  [-0.907931234308037, -5.89648750071951, -3.8822560318274855, 0.9000131667157665, 6.686296132500578, 4.512322662877144, 0.03133333333333334, -0.07866666666666665, -0.11400000000000002] \n",
      " distance =  7.117922159523411 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -8 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [5. 5. 5.]\n",
      "\n",
      "\n",
      "STATE =  [-0.907931234308037, -5.89648750071951, -3.8822560318274855, 0.9000131667157665, 6.686296132500578, 4.512322662877144, 0.03133333333333334, -0.07866666666666665, -0.11400000000000002]\n",
      "\n",
      " firstAngle =  15.599999999999994  secondAngle =  -24.239999999999995  thirdAngle =  -45.599999999999994 \n",
      "\n",
      "not normalize end pos: [1.451993811419418, 7.15093240891122, 5.200457456729589]\n",
      "\n",
      " old state =  [-0.907931234308037, -5.89648750071951, -3.8822560318274855, 0.9000131667157665, 6.686296132500578, 4.512322662877144, 0.03133333333333334, -0.07866666666666665, -0.11400000000000002] \n",
      " action =  [5. 5. 5.] \n",
      " new state =  [-1.4599118790116885, -6.361123777130151, -4.570390825679931, 1.451993811419418, 7.15093240891122, 5.200457456729589, 0.04333333333333332, -0.06733333333333331, -0.12666666666666665] \n",
      " distance =  7.967666578234137 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -9 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [4.9983983 5.        5.       ]\n",
      "\n",
      "\n",
      "STATE =  [-1.4599118790116885, -6.361123777130151, -4.570390825679931, 1.451993811419418, 7.15093240891122, 5.200457456729589, 0.04333333333333332, -0.06733333333333331, -0.12666666666666665]\n",
      "\n",
      " firstAngle =  20.639999999999986  secondAngle =  -20.159999999999997  thirdAngle =  -51.120000000000005 \n",
      "\n",
      "not normalize end pos: [2.2145030490875897, 7.734426583967185, 5.879124278500886]\n",
      "\n",
      " old state =  [-1.4599118790116885, -6.361123777130151, -4.570390825679931, 1.451993811419418, 7.15093240891122, 5.200457456729589, 0.04333333333333332, -0.06733333333333331, -0.12666666666666665] \n",
      " action =  [4.9983983 5.        5.       ] \n",
      " new state =  [-2.22242111667986, -6.944617952186117, -5.249057647451228, 2.2145030490875897, 7.734426583967185, 5.879124278500886, 0.0573333333333333, -0.05599999999999999, -0.14200000000000002] \n",
      " distance =  8.984402056228143 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -10 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-4.99999332  4.92942524  5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.22242111667986, -6.944617952186117, -5.249057647451228, 2.2145030490875897, 7.734426583967185, 5.879124278500886, 0.0573333333333333, -0.05599999999999999, -0.14200000000000002]\n",
      "\n",
      " firstAngle =  16.080000000000013  secondAngle =  -15.599999999999994  thirdAngle =  -56.4 \n",
      "\n",
      "not normalize end pos: [2.0038296849961714, 8.158763863447476, 6.951537226670727]\n",
      "\n",
      " old state =  [-2.22242111667986, -6.944617952186117, -5.249057647451228, 2.2145030490875897, 7.734426583967185, 5.879124278500886, 0.0573333333333333, -0.05599999999999999, -0.14200000000000002] \n",
      " action =  [-4.99999332  4.92942524  5.        ] \n",
      " new state =  [-2.011747752588442, -7.3689552316664075, -6.321470595621069, 2.0038296849961714, 8.158763863447476, 6.951537226670727, 0.0446666666666667, -0.04333333333333332, -0.15666666666666668] \n",
      " distance =  9.915120812055198 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -11 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [ 4.9202528  -4.88969135  5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.011747752588442, -7.3689552316664075, -6.321470595621069, 2.0038296849961714, 8.158763863447476, 6.951537226670727, 0.0446666666666667, -0.04333333333333332, -0.15666666666666668]\n",
      "\n",
      " firstAngle =  20.639999999999986  secondAngle =  -21.120000000000005  thirdAngle =  -61.44 \n",
      "\n",
      "not normalize end pos: [2.3206588353921433, 9.769567080459218, 6.160949612100491]\n",
      "\n",
      " old state =  [-2.011747752588442, -7.3689552316664075, -6.321470595621069, 2.0038296849961714, 8.158763863447476, 6.951537226670727, 0.0446666666666667, -0.04333333333333332, -0.15666666666666668] \n",
      " action =  [ 4.9202528  -4.88969135  5.        ] \n",
      " new state =  [-2.328576902984414, -8.97975844867815, -5.530882981050833, 2.3206588353921433, 9.769567080459218, 6.160949612100491, 0.0573333333333333, -0.05866666666666668, -0.17066666666666666] \n",
      " distance =  10.800416600288937 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -12 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-4.99999475  5.          5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.328576902984414, -8.97975844867815, -5.530882981050833, 2.3206588353921433, 9.769567080459218, 6.160949612100491, 0.0573333333333333, -0.05866666666666668, -0.17066666666666666]\n",
      "\n",
      " firstAngle =  15.840000000000003  secondAngle =  -17.040000000000006  thirdAngle =  -66.96000000000001 \n",
      "\n",
      "not normalize end pos: [2.026195840133044, 10.323391811797482, 7.141390892012484]\n",
      "\n",
      " old state =  [-2.328576902984414, -8.97975844867815, -5.530882981050833, 2.3206588353921433, 9.769567080459218, 6.160949612100491, 0.0573333333333333, -0.05866666666666668, -0.17066666666666666] \n",
      " action =  [-4.99999475  5.          5.        ] \n",
      " new state =  [-2.0341139077253145, -9.533583180016413, -6.511324260962826, 2.026195840133044, 10.323391811797482, 7.141390892012484, 0.04400000000000001, -0.04733333333333335, -0.18600000000000003] \n",
      " distance =  11.722805605796614 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -13 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [4.99924612 5.         5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.0341139077253145, -9.533583180016413, -6.511324260962826, 2.026195840133044, 10.323391811797482, 7.141390892012484, 0.04400000000000001, -0.04733333333333335, -0.18600000000000003]\n",
      "\n",
      " firstAngle =  20.639999999999986  secondAngle =  -12.480000000000004  thirdAngle =  -72.24000000000001 \n",
      "\n",
      "not normalize end pos: [2.946908005914379, 10.704204900090323, 7.8235333255549735]\n",
      "\n",
      " old state =  [-2.0341139077253145, -9.533583180016413, -6.511324260962826, 2.026195840133044, 10.323391811797482, 7.141390892012484, 0.04400000000000001, -0.04733333333333335, -0.18600000000000003] \n",
      " action =  [4.99924612 5.         5.        ] \n",
      " new state =  [-2.9548260735066494, -9.914396268309254, -7.193466694505315, 2.946908005914379, 10.704204900090323, 7.8235333255549735, 0.0573333333333333, -0.03466666666666668, -0.2006666666666667] \n",
      " distance =  12.600484656341454 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -14 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-4.99999475  5.          5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.9548260735066494, -9.914396268309254, -7.193466694505315, 2.946908005914379, 10.704204900090323, 7.8235333255549735, 0.0573333333333333, -0.03466666666666668, -0.2006666666666667]\n",
      "\n",
      " firstAngle =  15.599999999999994  secondAngle =  -8.159999999999997  thirdAngle =  -77.03999999999999 \n",
      "\n",
      "not normalize end pos: [2.4897044995934037, 10.958050485072317, 8.917119500190298]\n",
      "\n",
      " old state =  [-2.9548260735066494, -9.914396268309254, -7.193466694505315, 2.946908005914379, 10.704204900090323, 7.8235333255549735, 0.0573333333333333, -0.03466666666666668, -0.2006666666666667] \n",
      " action =  [-4.99999475  5.          5.        ] \n",
      " new state =  [-2.497622567185674, -10.168241853291248, -8.28705286914064, 2.4897044995934037, 10.958050485072317, 8.917119500190298, 0.04333333333333332, -0.022666666666666658, -0.21399999999999997] \n",
      " distance =  13.35314592637522 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -15 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [4.9997282 5.        5.       ]\n",
      "\n",
      "\n",
      "STATE =  [-2.497622567185674, -10.168241853291248, -8.28705286914064, 2.4897044995934037, 10.958050485072317, 8.917119500190298, 0.04333333333333332, -0.022666666666666658, -0.21399999999999997]\n",
      "\n",
      " firstAngle =  20.159999999999997  secondAngle =  -3.3599999999999994  thirdAngle =  -82.32 \n",
      "\n",
      "not normalize end pos: [3.5379093286236456, 11.150776998971203, 9.636510860276006]\n",
      "\n",
      " old state =  [-2.497622567185674, -10.168241853291248, -8.28705286914064, 2.4897044995934037, 10.958050485072317, 8.917119500190298, 0.04333333333333332, -0.022666666666666658, -0.21399999999999997] \n",
      " action =  [4.9997282 5.        5.       ] \n",
      " new state =  [-3.545827396215916, -10.360968367190134, -9.006444229226348, 3.5379093286236456, 11.150776998971203, 9.636510860276006, 0.05599999999999999, -0.009333333333333332, -0.22866666666666666] \n",
      " distance =  14.1788079570828 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -16 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.         -1.68027735  5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-3.545827396215916, -10.360968367190134, -9.006444229226348, 3.5379093286236456, 11.150776998971203, 9.636510860276006, 0.05599999999999999, -0.009333333333333332, -0.22866666666666666]\n",
      "\n",
      " firstAngle =  14.879999999999995  secondAngle =  -5.280000000000001  thirdAngle =  -86.88 \n",
      "\n",
      "not normalize end pos: [2.5391697960781885, 12.363674580632107, 9.556324781048062]\n",
      "\n",
      " old state =  [-3.545827396215916, -10.360968367190134, -9.006444229226348, 3.5379093286236456, 11.150776998971203, 9.636510860276006, 0.05599999999999999, -0.009333333333333332, -0.22866666666666666] \n",
      " action =  [-5.         -1.68027735  5.        ] \n",
      " new state =  [-2.547087863670459, -11.573865948851038, -8.926258149998404, 2.5391697960781885, 12.363674580632107, 9.556324781048062, 0.04133333333333332, -0.01466666666666667, -0.24133333333333332] \n",
      " distance =  14.836445468765213 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -17 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [5. 5. 5.]\n",
      "\n",
      "\n",
      "STATE =  [-2.547087863670459, -11.573865948851038, -8.926258149998404, 2.5391697960781885, 12.363674580632107, 9.556324781048062, 0.04133333333333332, -0.01466666666666667, -0.24133333333333332]\n",
      "\n",
      " firstAngle =  19.680000000000007  secondAngle =  -0.23999999999999488  thirdAngle =  -92.64 \n",
      "\n",
      "not normalize end pos: [3.68282608402232, 12.552582224351115, 10.297058536617833]\n",
      "\n",
      " old state =  [-2.547087863670459, -11.573865948851038, -8.926258149998404, 2.5391697960781885, 12.363674580632107, 9.556324781048062, 0.04133333333333332, -0.01466666666666667, -0.24133333333333332] \n",
      " action =  [5. 5. 5.] \n",
      " new state =  [-3.6907441516145907, -11.762773592570046, -9.666991905568175, 3.68282608402232, 12.552582224351115, 10.297058536617833, 0.05466666666666668, -0.0006666666666666524, -0.25733333333333336] \n",
      " distance =  15.666370590697168 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -18 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [-3.6907441516145907, -11.762773592570046, -9.666991905568175, 3.68282608402232, 12.552582224351115, 10.297058536617833, 0.05466666666666668, -0.0006666666666666524, -0.25733333333333336]\n",
      "\n",
      " firstAngle =  14.400000000000006  secondAngle =  4.799999999999997  thirdAngle =  -97.44 \n",
      "\n",
      "not normalize end pos: [2.9824033618593266, 12.464578601443426, 11.615694184120398]\n",
      "\n",
      " old state =  [-3.6907441516145907, -11.762773592570046, -9.666991905568175, 3.68282608402232, 12.552582224351115, 10.297058536617833, 0.05466666666666668, -0.0006666666666666524, -0.25733333333333336] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-2.990321429451597, -11.674769969662357, -10.98562755307074, 2.9824033618593266, 12.464578601443426, 11.615694184120398, 0.040000000000000015, 0.013333333333333326, -0.27066666666666667] \n",
      " distance =  16.30724651284681 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -19 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [4.99975777 1.03476882 5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-2.990321429451597, -11.674769969662357, -10.98562755307074, 2.9824033618593266, 12.464578601443426, 11.615694184120398, 0.040000000000000015, 0.013333333333333326, -0.27066666666666667]\n",
      "\n",
      " firstAngle =  19.19999999999999  secondAngle =  5.040000000000006  thirdAngle =  -102.0 \n",
      "\n",
      "not normalize end pos: [3.9375714196933025, 13.286543553844224, 11.307164900296778]\n",
      "\n",
      " old state =  [-2.990321429451597, -11.674769969662357, -10.98562755307074, 2.9824033618593266, 12.464578601443426, 11.615694184120398, 0.040000000000000015, 0.013333333333333326, -0.27066666666666667] \n",
      " action =  [4.99975777 1.03476882 5.        ] \n",
      " new state =  [-3.945489487285573, -12.496734922063155, -10.67709826924712, 3.9375714196933025, 13.286543553844224, 11.307164900296778, 0.0533333333333333, 0.014000000000000018, -0.2833333333333333] \n",
      " distance =  16.903718480197 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -20 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5.  5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [-3.945489487285573, -12.496734922063155, -10.67709826924712, 3.9375714196933025, 13.286543553844224, 11.307164900296778, 0.0533333333333333, 0.014000000000000018, -0.2833333333333333]\n",
      "\n",
      " firstAngle =  13.680000000000007  secondAngle =  9.360000000000014  thirdAngle =  -107.76 \n",
      "\n",
      "not normalize end pos: [3.0351433771882, 13.447144644680941, 12.469573471070698]\n",
      "\n",
      " old state =  [-3.945489487285573, -12.496734922063155, -10.67709826924712, 3.9375714196933025, 13.286543553844224, 11.307164900296778, 0.0533333333333333, 0.014000000000000018, -0.2833333333333333] \n",
      " action =  [-5.  5.  5.] \n",
      " new state =  [-3.0430614447804705, -12.657336012899872, -11.83950684002104, 3.0351433771882, 13.447144644680941, 12.469573471070698, 0.03800000000000002, 0.026000000000000037, -0.29933333333333334] \n",
      " distance =  17.596655935576692 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -21 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [4.99999714 4.73406649 5.        ]\n",
      "\n",
      "\n",
      "STATE =  [-3.0430614447804705, -12.657336012899872, -11.83950684002104, 3.0351433771882, 13.447144644680941, 12.469573471070698, 0.03800000000000002, 0.026000000000000037, -0.29933333333333334]\n",
      "\n",
      " firstAngle =  18.24000000000001  secondAngle =  13.919999999999987  thirdAngle =  -113.52 \n",
      "\n",
      "not normalize end pos: [4.298326719092726, 13.482047018291455, 13.042739496873727]\n",
      "\n",
      " old state =  [-3.0430614447804705, -12.657336012899872, -11.83950684002104, 3.0351433771882, 13.447144644680941, 12.469573471070698, 0.03800000000000002, 0.026000000000000037, -0.29933333333333334] \n",
      " action =  [4.99999714 4.73406649 5.        ] \n",
      " new state =  [-4.306244786684997, -12.692238386510386, -12.412672865824069, 4.298326719092726, 13.482047018291455, 13.042739496873727, 0.05066666666666669, 0.038666666666666634, -0.3153333333333333] \n",
      " distance =  18.267761414492597 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -22 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [-5. -5.  5.]\n",
      "\n",
      "\n",
      "STATE =  [-4.306244786684997, -12.692238386510386, -12.412672865824069, 4.298326719092726, 13.482047018291455, 13.042739496873727, 0.05066666666666669, 0.038666666666666634, -0.3153333333333333]\n",
      "\n",
      " firstAngle =  13.439999999999998  secondAngle =  9.120000000000005  thirdAngle =  -118.8 \n",
      "\n",
      "not normalize end pos: [2.849439046468052, 15.552735097621472, 11.923761909168148]\n",
      "\n",
      " old state =  [-4.306244786684997, -12.692238386510386, -12.412672865824069, 4.298326719092726, 13.482047018291455, 13.042739496873727, 0.05066666666666669, 0.038666666666666634, -0.3153333333333333] \n",
      " action =  [-5. -5.  5.] \n",
      " new state =  [-2.8573571140603224, -14.762926465840403, -11.29369527811849, 2.849439046468052, 15.552735097621472, 11.923761909168148, 0.03733333333333333, 0.025333333333333347, -0.33] \n",
      " distance =  18.805744881500388 \n",
      " current reward =  -1 \n",
      " cumulative reward =  -23 \n",
      "\n",
      "\n",
      "\n",
      "ACTION =  [5. 5. 5.]\n",
      "\n",
      "\n",
      "STATE =  [-2.8573571140603224, -14.762926465840403, -11.29369527811849, 2.849439046468052, 15.552735097621472, 11.923761909168148, 0.03733333333333333, 0.025333333333333347, -0.33]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1\n",
    "\n",
    "servo1 = Servo()\n",
    "servo2 = Servo()\n",
    "servo3 = Servo()\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/without_her_rotate_scaled.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "\n",
    "    \n",
    "    state, goal = read_state()\n",
    "    \n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action([state], [goal], 0)\n",
    "        print(\"\\n\\nACTION = \", action)\n",
    "        print(\"\\n\\nSTATE = \", state)\n",
    "        # применяем углы к сервам\n",
    "        #servo1, servo2, servo3 = set_action_test(action, servo1, servo2, servo3)\n",
    "        \n",
    "        set_action(action)\n",
    "        \n",
    "        # как-то ждем, пока сервы повернутся\n",
    "        #time.sleep(5)\n",
    "        \n",
    "        # получаем обновленный state\n",
    "        next_state, next_goal = read_state()\n",
    "        \n",
    "        done, distance = check_if_done(next_state[3:6], goal)\n",
    "        \n",
    "        reward = -1\n",
    "        \n",
    "        if done:\n",
    "            reward += 10\n",
    "        \n",
    "        r += reward\n",
    "        \n",
    "        print(\"\\n old state = \", state, \"\\n action = \", action, \"\\n new state = \", next_state, \"\\n distance = \", distance,\n",
    "              \"\\n current reward = \", reward, \"\\n cumulative reward = \", r, \"\\n\")\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403f8cc-1b3c-4e5b-bdb5-156d2580f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
